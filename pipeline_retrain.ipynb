{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68126e9-cb97-43a5-8ba9-b0ee4f23714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import init\n",
    "import math\n",
    "from math import gcd\n",
    "from math import sqrt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class PHMLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features, out_features,n=2):\n",
    "    super(PHMLayer, self).__init__()\n",
    "    self.n = n\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "\n",
    "    self.bias = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "    self.a = torch.zeros((n, n, n))\n",
    "    self.a = Parameter(torch.nn.init.xavier_uniform_(self.a))\n",
    "\n",
    "    self.s = torch.zeros((n, self.out_features//n, self.in_features//n)) \n",
    "    self.s = Parameter(torch.nn.init.xavier_uniform_(self.s))\n",
    "\n",
    "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
    "\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "  def kronecker_product1(self, a, b):\n",
    "\n",
    "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
    "    \n",
    "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
    "    siz0 = res.shape[:-4]\n",
    "    out = res.reshape(siz0 + siz1)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
    "\n",
    "    input = input.type(dtype=self.weight.type())\n",
    "\n",
    "      \n",
    "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'in_features={}, out_features={}, bias={}'.format(\n",
    "      self.in_features, self.out_features, self.bias is not None)\n",
    "    \n",
    "  def reset_parameters(self) -> None:\n",
    "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
    "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6a55ff-cf0a-4e2c-8142-fe33b676889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "class Model_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout):\n",
    "        super(Model_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "        self.Drop = nn.Dropout(self.dropout)\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        \n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_dim),\n",
    "            # nn.Dropout(self.dropout),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0]\n",
    "\n",
    "        last_hidden_state_cls = self.linear(last_hidden_state_cls)\n",
    "\n",
    "        last_hidden_state_cls = self.Drop(last_hidden_state_cls)\n",
    "        \n",
    "\n",
    "        logits = self.linear2(last_hidden_state_cls)[:, 0, :]\n",
    "\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits, last_hidden_state_cls,outputs[0]\n",
    "class QModel_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout,feature_remove_max= True):\n",
    "        super(QModel_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "        divisors = sorted(self.cf(embedding_dim,hidden_dim))\n",
    "        divisors1 = sorted(self.cf(hidden_dim,num_labels))\n",
    "        common_divisors = sorted(set(divisors1) & set(divisors))\n",
    "        if(feature_remove_max == True):\n",
    "            self.n = common_divisors[-1]\n",
    "        else :\n",
    "            self.n = common_divisors[0]\n",
    "        \n",
    "        self.linear = PHMLayer(self.embedding_dim, self.hidden_dim,self.n)\n",
    "        self.Drop = nn.Dropout(self.dropout)\n",
    "        self.linear2 = PHMLayer(self.hidden_dim, self.num_labels,self.n)\n",
    "        \n",
    "\n",
    "    def cf(self,num1,num2):\n",
    "            n=[]\n",
    "            g=gcd(num1, num2)\n",
    "            for i in range(1, int(sqrt(g))+1):\n",
    "                if g%i==0:\n",
    "                    n.append(i)\n",
    "                    if g!=i*i:\n",
    "                        n.append(int(g/i))\n",
    "            return n\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "\n",
    "        last_hidden_state_cls = outputs[0]\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "        last_hidden_state_cls = self.linear(last_hidden_state_cls)\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "        last_hidden_state_cls = self.Drop(last_hidden_state_cls)\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "\n",
    "        logits = self.linear2(last_hidden_state_cls)[:, 0, :]\n",
    "        #print(logits.shape)\n",
    "        # Feed input to classifier to compute logits\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits, last_hidden_state_cls,outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2537bbee-dd05-436f-9439-13bec012ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import nn, optim, tensor\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    # torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "# set parameters\n",
    "\n",
    "\n",
    "scl_model_path = r\"itmo_model.pt\"\n",
    "cross_model_path = r\"itmo_model.pt\"\n",
    "\n",
    "\n",
    "# a function for preprocessing text\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(tokenizer,data, MAX_LEN):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,  # Pad sentence to max length\n",
    "            return_attention_mask=True  # Return attention mask\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "# preparing data\n",
    "def prepare_data(tokenizer,train_ds,val_ds=None,aug_path = None,sample_num = 10 , seed = 32, all=True , aug = False,aug_num = 6):\n",
    "    # load data\n",
    "    # for tsv\n",
    "    # there is no valid_path\n",
    "    \n",
    "\n",
    "      \n",
    "    global num_classes\n",
    "    num_classes = len(train_ds['label'].unique())\n",
    "    \n",
    "    #original one\n",
    "    if(all == False):\n",
    "        train_df = [train_ds.loc[train_ds.label == i].sample(n=sample, random_state=seed) for i in\n",
    "                    train_ds.label.unique()]\n",
    "        train_df = pd.concat(train_df, axis=0).sample(frac=1)\n",
    "    else :\n",
    "        #for all\n",
    "        train_df = train_ds\n",
    "    \n",
    "    train_df = train_df[['sentence','label']]\n",
    "    print(train_df.head(3))\n",
    "    # data augmentation \n",
    "    if(aug == True):\n",
    "        indexs = train_df.index.values.tolist()\n",
    "        aug_ds =  pd.read_csv(aug_path, sep='\\t')\n",
    "        aug_df = [aug_ds[i*aug_num:i*aug_num+aug_num] for i in\n",
    "                            indexs]\n",
    "        \n",
    "        print(aug_df[:aug_num*2])\n",
    "        aug_df = pd.concat(aug_df, axis=0).sample(frac=1)\n",
    "\n",
    "        train_df = pd.concat([train_df,aug_df], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    if(val_ds == None):\n",
    "        sample = 5\n",
    "        \n",
    "        val_df = [train_df.loc[train_df.label == i].sample(n=sample, random_state=seed) for i in\n",
    "                    train_df.label.unique()]\n",
    "        val_df = pd.concat(val_df, axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    # random 20 per class sample for validation\n",
    "\n",
    "\n",
    "    \n",
    "    train_text = train_df[\"sentence\"]\n",
    "    train_label = train_df[\"label\"]\n",
    "    val_text = val_df[\"sentence\"]\n",
    "    val_label = val_df[\"label\"]\n",
    "    \n",
    "\n",
    "    # Concatenate train data and test data\n",
    "    all_text = np.concatenate([train_text, val_text], axis=0)\n",
    "\n",
    "    # Encode our concatenated data\n",
    "    encoded_text = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_text]\n",
    "    global MAX_LEN\n",
    "    # Find the maximum length\n",
    "    MAX_LEN = max([len(sent) for sent in encoded_text])\n",
    "\n",
    "    # preprocessing train data\n",
    "    for i in range(len(train_text)):\n",
    "        train_text[i] = text_preprocessing(train_text[i])\n",
    "\n",
    "    # preprocessing validation data\n",
    "    for i in range(len(val_text)):\n",
    "        val_text[i] = text_preprocessing(val_text[i])\n",
    "\n",
    "    # Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "    # print('Tokenizing data...')\n",
    "    train_inputs, train_masks = preprocessing_for_bert(tokenizer,train_text, MAX_LEN)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(tokenizer,val_text, MAX_LEN)\n",
    "\n",
    "\n",
    "    # Convert other data types to torch.Tensor\n",
    "    train_labels = torch.tensor(train_label)\n",
    "    val_labels = torch.tensor(val_label)\n",
    "\n",
    "\n",
    "    # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "    batch_size = 16\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "\n",
    "def initialize_model(model,hidden = 16 , num_labels = 2 ,feature_remove_max=True):\n",
    "    \"\"\"Initialize the Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate Bert Classifier\n",
    "    if(model == QModel_Classifier):\n",
    "        model_classifier = model(1024, hidden_dim=hidden, num_labels = num_labels, dropout=0.1,feature_remove_max=feature_remove_max)\n",
    "    else:\n",
    "        model_classifier = model(1024, hidden_dim=hidden, num_labels = num_labels, dropout=0.1)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    model_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(model_classifier.parameters(),\n",
    "                      lr=4e-5,  # Default learning rate\n",
    "                      eps=1e-8  # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    return model_classifier, optimizer\n",
    "\n",
    "\n",
    "def contrastive_loss(temp, embedding, label):\n",
    "    \"\"\"calculate the contrastive loss\n",
    "    \"\"\"\n",
    "    nsamples, nx, ny = embedding.shape\n",
    "    embedding = embedding.reshape((nsamples,nx*ny))\n",
    "    \n",
    "    # cosine similarity between embeddings\n",
    "    cosine_sim = cosine_similarity(embedding, embedding)\n",
    "    # remove diagonal elements from matrix\n",
    "    dis = cosine_sim[~np.eye(cosine_sim.shape[0], dtype=bool)].reshape(cosine_sim.shape[0], -1)\n",
    "    # apply temprature to elements\n",
    "    dis = dis / temp\n",
    "    cosine_sim = cosine_sim / temp\n",
    "    # apply exp to elements\n",
    "    dis = np.exp(dis)\n",
    "    cosine_sim = np.exp(cosine_sim)\n",
    "\n",
    "    # calculate row sum\n",
    "    row_sum = []\n",
    "    for i in range(len(embedding)):\n",
    "        row_sum.append(sum(dis[i]))\n",
    "    # calculate outer sum\n",
    "    contrastive_loss = 0\n",
    "    for i in range(len(embedding)):\n",
    "        n_i = label.tolist().count(label[i]) - 1\n",
    "        inner_sum = 0\n",
    "        # calculate inner sum\n",
    "        for j in range(len(embedding)):\n",
    "            if label[i] == label[j] and i != j:\n",
    "                inner_sum = inner_sum + np.log(cosine_sim[i][j] / row_sum[i])\n",
    "        if n_i != 0:\n",
    "            contrastive_loss += (inner_sum / (-n_i))\n",
    "        else:\n",
    "            contrastive_loss += 0\n",
    "    return contrastive_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader, tem, lam, scl):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, h_s,_ = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        if scl:\n",
    "            cross_loss = loss_fn(logits, b_labels)\n",
    "            contrastive_l = contrastive_loss(tem, h_s.cpu().detach().numpy(), b_labels)\n",
    "            loss = (lam * contrastive_l) + (1 - lam) * (cross_loss)\n",
    "            val_loss.append(loss.item())\n",
    "        else:\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            #self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, tem, lam, scl,epoch = 40,val_dataloader=None, evaluation=False,patience = 25):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Specify loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    val_list = []\n",
    "    train_list = []\n",
    "    best_validation_loss = float('inf')\n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta=0)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        e = e + 1\n",
    "        print(\n",
    "            f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Train Accuracy':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\" * 86)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        train_accuracy = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits, hiden_state,_ = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the accuracy rate\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            train_accuracy.append(accuracy)\n",
    "\n",
    "            # Compute loss\n",
    "            if scl == True:\n",
    "                cross_loss = loss_fn(logits, b_labels)\n",
    "                contrastive_l = contrastive_loss(tem, hiden_state.cpu().detach().numpy(), b_labels)\n",
    "                loss = (lam * contrastive_l) + (1 - lam) * (cross_loss)\n",
    "            if scl == False:\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            print(\n",
    "                f\"{e:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {accuracy:^14.6} | {'-':^10} | {'-':^9} | {'-':^9.2}\")\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "\n",
    "        # Reset batch tracking variables\n",
    "        batch_loss, batch_counts = 0, 0\n",
    "        t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        total_accuracy = np.mean(train_accuracy)\n",
    "        train_list.append(avg_train_loss)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader, tem, lam, scl)\n",
    "            val_list.append(val_loss)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(\"-\" * 86)\n",
    "            print(\n",
    "                f\"{'end':^7} | {'-':^7} | {avg_train_loss:^12.6f} | {total_accuracy:^14.6} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 86)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "        \n",
    "        if (val_loss < best_validation_loss) and scl == True:\n",
    "            best_validation_loss = val_loss\n",
    "            torch.save(model.state_dict(), scl_model_path)\n",
    "        elif (val_loss < best_validation_loss) and scl == False:\n",
    "            best_validation_loss = val_loss\n",
    "            torch.save(model.state_dict(), cross_model_path)\n",
    "\n",
    "\n",
    "        #early stopping\n",
    "        #print(early_stopper.counter)\n",
    "        if early_stopper.early_stop(val_loss):  \n",
    "            break\n",
    "\n",
    "\n",
    "    # plot train and valid loss\n",
    "    plt.plot(list(range(len(val_list))), val_list, label=\"validation loss\")\n",
    "    plt.plot(list(range(len(train_list))), train_list, label=\"training loss\")\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('number of epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    return best_validation_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def test_evaluate(model,model_path, test_dataloader,hidden=16,num_labels=2,feature_remove_max=False):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our vtest set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    if(model == QModel_Classifier):\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1,feature_remove_max=feature_remove_max)\n",
    "    else:\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_accuracy = []\n",
    "    predict = []\n",
    "    y_true = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, _ ,_= model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predict += preds.tolist()\n",
    "        y_true += b_labels.tolist()\n",
    "\n",
    "    # plot heatmap\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "    cm = confusion_matrix(y_true, predict)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sn.heatmap(cm, annot=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    print(f'Accuracy: {accuracy_score(y_true, predict)}')\n",
    "\n",
    "    # Recall\n",
    "    print(f'Recall: {recall_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # Precision\n",
    "    print(f'Precision: {precision_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # F1_score\n",
    "    print(f'F1_score: {f1_score(y_true, predict, average=None)}')\n",
    "\n",
    "    return accuracy_score(y_true, predict)\n",
    "    \n",
    "#test_evaluate(cross_model_path, test_dataloader)\n",
    "#test_evaluate(scl_model_path, test_dataloader)\n",
    "\n",
    "# scl_test_acc = test_evaluate(scl_model_path, test_dataloader)\n",
    "\n",
    "def model_predict(model,hidden, model_path, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model = model(1024,hidden, num_classes, dropout=0.1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, _,_ = model(b_input_ids, b_attn_mask)\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        all_logits += preds.tolist()\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    # all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    # probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    # predict = np.argmax(probs)\n",
    "\n",
    "    return all_logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8641f267-1263-4825-8fce-da6ad5402ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What are the requirements to enter the program?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>how to enter program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>what do I need to enter the program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>how can I get scholarships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the program offer any scholarships to int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>21</td>\n",
       "      <td>What do I need to do to get an internship?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>22</td>\n",
       "      <td>is there club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>22</td>\n",
       "      <td>How many clubs are currently operating in this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>22</td>\n",
       "      <td>How many active clubs are there at this univer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>22</td>\n",
       "      <td>how do I join club in university</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                           sentence\n",
       "0       0    What are the requirements to enter the program?\n",
       "1       0                               how to enter program\n",
       "2       0                what do I need to enter the program\n",
       "3       1                         how can I get scholarships\n",
       "4       1  Does the program offer any scholarships to int...\n",
       "..    ...                                                ...\n",
       "77     21         What do I need to do to get an internship?\n",
       "78     22                                      is there club\n",
       "79     22  How many clubs are currently operating in this...\n",
       "80     22  How many active clubs are there at this univer...\n",
       "81     22                   how do I join club in university\n",
       "\n",
       "[82 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "eastern_tz = pytz.timezone('Europe/Moscow')\n",
    "token = 'mongodb+srv://mongo:mongo@cluster0.gcj8po2.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "\n",
    "myclient = pymongo.MongoClient(token)\n",
    "mydb = myclient[\"itmo_data\"]\n",
    "mycol = mydb['training_data']\n",
    "\n",
    "x= mycol.find().sort('tag')\n",
    "df =  pd.DataFrame(list(x))\n",
    "df = df[['tag','patterns']]\n",
    "df = df.rename(columns={'tag':'label','patterns':'sentence'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f935489-521d-484d-b1fe-928b7ec17d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103f529c-36a3-4e0e-95d1-e4a340228705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eda phase\n",
      "generated augmented sentences with eda for to main_bot/eda.tsv with num_aug=6\n",
      "GPT phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.6_1/libexec/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cPosGpt2 training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 2.098106062412262\n",
      "Epoch 0, Dev loss 5.357265949249268\n",
      "Saving model. Best dev so far 5.357265949249268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▌                                | 1/10 [05:40<51:01, 340.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 1.2324367880821228\n",
      "Epoch 1, Dev loss 4.608185291290283\n",
      "Saving model. Best dev so far 4.608185291290283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|███████▏                            | 2/10 [11:55<48:08, 361.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.9908360052108764\n",
      "Epoch 2, Dev loss 4.0505512952804565\n",
      "Saving model. Best dev so far 4.0505512952804565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|██████████▊                         | 3/10 [18:03<42:28, 364.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.8665652668476105\n",
      "Epoch 3, Dev loss 3.9272326231002808\n",
      "Saving model. Best dev so far 3.9272326231002808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|██████████████▍                     | 4/10 [23:22<34:36, 346.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.7712208855152131\n",
      "Epoch 4, Dev loss 3.7603291273117065\n",
      "Saving model. Best dev so far 3.7603291273117065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|██████████████████                  | 5/10 [29:21<29:14, 350.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.678541356921196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|█████████████████████▌              | 6/10 [34:10<21:59, 329.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Dev loss 3.833557367324829\n",
      "avg_loss: 0.6046176505088806\n",
      "Epoch 6, Dev loss 3.689789652824402\n",
      "Saving model. Best dev so far 3.689789652824402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|█████████████████████████▏          | 7/10 [36:36<13:29, 269.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.5583619576692581\n",
      "Epoch 7, Dev loss 3.6848526000976562\n",
      "Saving model. Best dev so far 3.6848526000976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████████████████████████▊       | 8/10 [39:51<08:12, 246.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.5084008663892746\n",
      "Epoch 8, Dev loss 3.6762644052505493\n",
      "Saving model. Best dev so far 3.6762644052505493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|████████████████████████████████▍   | 9/10 [42:59<03:47, 227.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 0.47787390112876893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████| 10/10 [46:15<00:00, 277.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Dev loss 3.7634854316711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 per sentence is augmented, file is saved in <_io.TextIOWrapper name='main_bot/posgpt2_eda.tsv' mode='w' encoding='UTF-8'>\n",
      "pre-process phase\n",
      "                                          sentence  label\n",
      "0  what are the requirements to enroll the program      0\n",
      "1  what are the requirements to record the program      0\n",
      "2  what are the requirements to get in the program      0\n",
      "[    label                                           sentence\n",
      "0       0     what are the requirements to enter the program\n",
      "1       0   what are the requirements to enter the broadcast\n",
      "2       0     what are the requirements to enter the program\n",
      "3       0     what are the requirements to enter the program\n",
      "4       0    what are the requirements to enroll the program\n",
      "5       0  what are the requirements to enroll and improv...\n",
      "6       0  what are the requirements to enroll the programme\n",
      "7       0  what are the requirements to enroll the programme\n",
      "8       0  what are the requirements to inscribe the prog...\n",
      "9       0  what are the requirements to inscribe the program\n",
      "10      0  what are the requirements to inscribe the program\n",
      "11      0     what are the requirements to enrol the program\n",
      "12      0     what are the requirements to enrol the program\n",
      "13      0     what are the requirements to enrol the program\n",
      "14      0     what are the requirements to enrol the program\n",
      "15      0    what are the requirements to enroll the program\n",
      "16      0    what are the requirements to enroll the program\n",
      "17      0  what are the requirements to enroll the broadcast\n",
      "18      0  what are the requirements to enroll the broadcast\n",
      "19      0          what are the demand to enter the platform\n",
      "20      0          what are the demand to enroll the program\n",
      "21      0          what are the demand to enroll the program\n",
      "22      0     what are the necessary to record the programme\n",
      "23      0  what are the necessary to record the computer ...,     label                                           sentence\n",
      "24      0       what are the necessary to record the program\n",
      "25      0       what are the necessary to record the program\n",
      "26      0       what are the requirements to record the list\n",
      "27      0       what are the requirements to record the exam\n",
      "28      0       what are the requirements to record the plan\n",
      "29      0       what are the requirements to record the plan\n",
      "30      0  what are the requirements to read and postulat...\n",
      "31      0  what are the requirements to read and motorcar...\n",
      "32      0      what are the requirements to read the program\n",
      "33      0      what are the requirements to read the program\n",
      "34      0  what are the requirements to record the programme\n",
      "35      0  what are the requirements to record the programme\n",
      "36      0  what are the requirements to record the programme\n",
      "37      0  what are the requirements to record the programme\n",
      "38      0  what are the requirements to record the comput...\n",
      "39      0  what are the requirements to record the comput...\n",
      "40      0  what are the requirements to record the comput...\n",
      "41      0   what are the requirements to put down internship\n",
      "42      0  what are the requirements to put forward the t...\n",
      "43      0  what are the requirements to put down the program\n",
      "44      0  what are the requirements to put down the program\n",
      "45      0  what are the requirements to father in the bro...\n",
      "46      0  what are the requirements to father in the pro...\n",
      "47      0  what are the requirements to father in the pro...,     label                                           sentence\n",
      "48      0  what are the requirements to father in the pro...\n",
      "49      0           what are the requisite to get a capriole\n",
      "50      0  what are the requisite to get a line story in ...\n",
      "51      0       what are the requisite to get in the program\n",
      "52      0  what are the requirements to get into the program\n",
      "53      0    what are the requirements to get in the program\n",
      "54      0  what are the requirements to get in the programme\n",
      "55      0  what are the requirements to get in the programme\n",
      "56      0  what are the requirements to get into the program\n",
      "57      0  what are the requirements to get under ones sk...\n",
      "58      0  what are the requirements to get under ones sk...\n",
      "59      0    what are the requirements to get in the program\n",
      "60      0  what are the requirements to get in the comput...\n",
      "61      0  what are the requirements to get in the comput...\n",
      "62      0  what are the requirements to mother in the pro...\n",
      "63      0    what are the requirements to mother in the exam\n",
      "64      0  what are the requirements to mother in the pro...\n",
      "65      0  what are the requirements to mother in the pro...\n",
      "66      0  what are the demand to recruit the in this pro...\n",
      "67      0        what are the demand to get into the program\n",
      "68      0        what are the demand to get into the program\n",
      "69      0  what are the demand to enforce in the environment\n",
      "70      0          what are the demand to embark the program\n",
      "71      0          what are the demand to embark the program,     label                                           sentence\n",
      "72      0         what are the demand to enforce in the exam\n",
      "73      0            what are the demand to introduce thesis\n",
      "74      0       what are the demand to introduce the program\n",
      "75      0          what are the demand to embark the program\n",
      "76      0         what are the demand to enter the broadcast\n",
      "77      0          what are the demand to enter the platform\n",
      "78      0  what are the demand to participate the compute...\n",
      "79      0           what are the demand to enter the program\n",
      "80      0          what are the demand to enter the platform\n",
      "81      0  what are the demand to submit documents for en...\n",
      "82      0  what are the demand to participate the compute...\n",
      "83      0     what are the demand to participate the program\n",
      "84      0        what are the necessary to infix the program\n",
      "85      0        what are the necessary to infix the program\n",
      "86      0        what are the necessary to infix the program\n",
      "87      0  what are the necessary to infix the course of ...\n",
      "88      0  what are the necessary to infix the programme ...\n",
      "89      0        what are the necessary to infix the program\n",
      "90      0      what are the necessary to enter the broadcast\n",
      "91      0        what are the necessary to enter the program\n",
      "92      0      what are the necessary to enter the broadcast\n",
      "93      0      what are the necessary to enter the broadcast\n",
      "94      0        what are the necessity to adopt in the test\n",
      "95      0      what are the necessity to enter the broadcast,      label                                           sentence\n",
      "96       0        what are the necessity to enter the program\n",
      "97       0      what are the necessary to enter the programme\n",
      "98       0       what are the necessary to enter the syllabus\n",
      "99       0       what are the necessary to enter the syllabus\n",
      "100      0  what are the necessary to enter the calculator...\n",
      "101      0        what are the necessary to enter the program\n",
      "102      0       what are the necessary to enter the platform\n",
      "103      0       what are the necessary to enter the platform\n",
      "104      0  what are the requirements to bugger off in the...\n",
      "105      0  what are the requirements to bugger off an int...\n",
      "106      0  what are the requirements to bugger off in the...\n",
      "107      0  what are the requirements to bugger off in the...\n",
      "108      0  what are the requirements to produce the progr...\n",
      "109      0  what are the requirements to produce in the pr...\n",
      "110      0  what are the requirements to produce in the pr...\n",
      "111      0  what are the requirements to produce in the pr...\n",
      "112      0  what are the requirements to scram in the program\n",
      "113      0         what are the requirements to scram program\n",
      "114      0  what are the requirements to scram in the program\n",
      "115      0  what are the requirements to scram in the program\n",
      "116      0    what are the requirements to get in the program\n",
      "117      0    what are the requirements to get in the program\n",
      "118      0  what are the requirements to get in the programme\n",
      "119      0  what are the requirements to get in the programme,      label                                           sentence\n",
      "120      0  what are the prerequisite for admission to the...\n",
      "121      0    what are the prerequisite to get in the program\n",
      "122      0    what are the prerequisite to get in the program\n",
      "123      0    what are the requirements to suffer in the test\n",
      "124      0  what are the requirements to suffer in the vir...\n",
      "125      0  what are the requirements to suffer in the pro...\n",
      "126      0  what are the requirements to suffer in the pro...\n",
      "127      0                              how to embark program\n",
      "128      0                              how to embark program\n",
      "129      0                                 how to embark plan\n",
      "130      0                                 how to embark plan\n",
      "131      0  how to move up computer programme in this broa...\n",
      "132      0                             how to move internship\n",
      "133      0                              how to move into plan\n",
      "134      0                              how to move into plan\n",
      "135      0                            how to enter internship\n",
      "136      0                                 how to enter theme\n",
      "137      0                                how to enter design\n",
      "138      0                             how to enter designate\n",
      "139      0                             how to enter programme\n",
      "140      0                    how to enter computer programme\n",
      "141      0                                  how to enter exam\n",
      "142      0                           how to enter data format\n",
      "143      0                               how to enter program,      label                                       sentence\n",
      "144      0                           how to enter program\n",
      "145      0                            how to enter design\n",
      "146      0                            how to enter design\n",
      "147      0      how to put forward registration documents\n",
      "148      0                 how to put together internship\n",
      "149      0             how to put down plan for enrolment\n",
      "150      0                           how to put down plan\n",
      "151      0            how to enroll thesis at the program\n",
      "152      0                  how to enroll course of study\n",
      "153      0  how to enroll course of study at this program\n",
      "154      0                           how to recruit pupil\n",
      "155      0                      how to recruit internship\n",
      "156      0                         how to recruit program\n",
      "157      0                         how to recruit program\n",
      "158      0                           how to enrol program\n",
      "159      0             how to enrol thesis at the program\n",
      "160      0                           how to enrol program\n",
      "161      0                           how to enrol program\n",
      "162      0                           how to enter program\n",
      "163      0                              how to enter exam\n",
      "164      0                           how to enter program\n",
      "165      0                           how to enter program\n",
      "166      0                           how to enter program\n",
      "167      0                           how to enter program,      label                                           sentence\n",
      "168      0                               how to enter program\n",
      "169      0                               how to enter program\n",
      "170      0               how to enroll thesis at this program\n",
      "171      0             how to enroll data processor programme\n",
      "172      0                    how to enroll political program\n",
      "173      0                    how to enroll political program\n",
      "174      0                               how to enter program\n",
      "175      0                 how to enter course of instruction\n",
      "176      0                       how to enter course of study\n",
      "177      0                          how to recruit internship\n",
      "178      0                              how to recruit thesis\n",
      "179      0                          how to recruit curriculum\n",
      "180      0                          how to recruit curriculum\n",
      "181      0                                 how to enter theme\n",
      "182      0                       how to enter course of study\n",
      "183      0                       how to enter course of study\n",
      "184      0                               how to enter program\n",
      "185      0                       how to enter course of study\n",
      "186      0                               how to enter program\n",
      "187      0                               how to enter program\n",
      "188      0                    how to introduce encyclopaedism\n",
      "189      0                        how to introduce internship\n",
      "190      0  how to introduce curriculum in english languag...\n",
      "191      0                        how to introduce curriculum,      label                                 sentence\n",
      "192      0              how to introduce internship\n",
      "193      0                  how to introduce thesis\n",
      "194      0              how to introduce curriculum\n",
      "195      0              how to introduce curriculum\n",
      "196      0                   how to platter program\n",
      "197      0                   how to platter program\n",
      "198      0                   how to platter program\n",
      "199      0  how to register text file for admission\n",
      "200      0       how to register computer programme\n",
      "201      0                  how to register program\n",
      "202      0                  how to register program\n",
      "203      0               how to phonograph document\n",
      "204      0         how to phonograph record program\n",
      "205      0         how to phonograph record program\n",
      "206      0                   how to book internship\n",
      "207      0                   how to book internship\n",
      "208      0                      how to book program\n",
      "209      0                      how to book program\n",
      "210      0               how to record file cabinet\n",
      "211      0            how to record course of study\n",
      "212      0         how to record political platform\n",
      "213      0         how to record political platform\n",
      "214      0                    how to record program\n",
      "215      0         how to record computer programme,      label                                           sentence\n",
      "216      0                   how to record computer programme\n",
      "217      0                               how to enter program\n",
      "218      0                               how to enter program\n",
      "219      0                             how to enter broadcast\n",
      "220      0                             how to enter broadcast\n",
      "221      0                               how to enter program\n",
      "222      0                    how to enter computer programme\n",
      "223      0                    how to enter computer programme\n",
      "224      0                            how to enter curriculum\n",
      "225      0                               how to enter program\n",
      "226      0                              how to enter syllabus\n",
      "227      0                              how to enter syllabus\n",
      "228      0                 how to enter course of instruction\n",
      "229      0                       how to enter course of study\n",
      "230      0                            how to enter curriculum\n",
      "231      0                            how to enter curriculum\n",
      "232      0                           how to figure internship\n",
      "233      0                             how to figure out club\n",
      "234      0                            how to figure programme\n",
      "235      0                            how to figure programme\n",
      "236      0                          how to participate thesis\n",
      "237      0  how to participate course of study at this pro...\n",
      "238      0                       how to participate programme\n",
      "239      0                       how to participate programme,      label                                    sentence\n",
      "240      0      how to criminal record course of study\n",
      "241      0      how to criminal record course of study\n",
      "242      0              how to criminal record program\n",
      "243      0                    how to record internship\n",
      "244      0               how to record course of study\n",
      "245      0               how to record course of study\n",
      "246      0                  how to immortalize program\n",
      "247      0                  how to immortalize program\n",
      "248      0                  how to immortalize program\n",
      "249      0                  how to immortalize program\n",
      "250      0                  how to memorialize program\n",
      "251      0                  how to memorialize program\n",
      "252      0                  how to memorialize program\n",
      "253      0                  how to memorialize program\n",
      "254      0   how to read documents in english language\n",
      "255      0                            how to read list\n",
      "256      0                         how to read program\n",
      "257      0               how to read program text file\n",
      "258      0                  how to memorialise program\n",
      "259      0                  how to memorialise program\n",
      "260      0                  how to memorialise program\n",
      "261      0                  how to memorialise program\n",
      "262      0  what do i postulate to recruit the program\n",
      "263      0  what do i postulate to recruit the program,      label                                           sentence\n",
      "264      0         what do i postulate to recruit the program\n",
      "265      0          what do i need to do to enter the program\n",
      "266      0  what do i need to raise the ask for documents ...\n",
      "267      0                what do i need to raise the program\n",
      "268      0                what do i need to do to spawn a job\n",
      "269      0                what do i need to raise the program\n",
      "270      0                what do i need to raise the program\n",
      "271      0           what do i need to do to bugger off a job\n",
      "272      0              what do i need to recruit the program\n",
      "273      0                 what do i need to recruit the plan\n",
      "274      0          what do i call for to recruit the program\n",
      "275      0          what do i call for to recruit the program\n",
      "276      0          what do i call for to recruit the program\n",
      "277      0         what do i postulate to recruit the program\n",
      "278      0           what do i postulate to get an internship\n",
      "279      0         what do i postulate to recruit the program\n",
      "280      0        what do i motive to do to get an internship\n",
      "281      0            what do i motive to enter an internship\n",
      "282      0            what do i motive to enter the programme\n",
      "283      0  what do i indigence to do to baffle an internship\n",
      "284      0  what do i indigence to enter the computer prog...\n",
      "285      0         what do i indigence to enter the programme\n",
      "286      0          what do i need to do to get an internship\n",
      "287      0     what do i need to enter the computer programme,      label                                           sentence\n",
      "288      0              what do i need to enter the broadcast\n",
      "289      0      what do i ask to enter the computer programme\n",
      "290      0                 what do i ask to enter the program\n",
      "291      0               what do i ask to enter the programme\n",
      "292      0          what do i call for to recruit the program\n",
      "293      0           what do i call for to enter the platform\n",
      "294      0          what do i call for to enter the programme\n",
      "295      0            what do i need to recruit the programme\n",
      "296      0            what do i need to recruit the programme\n",
      "297      0            what do i need to recruit the programme\n",
      "298      0     what do i need to enter the computer programme\n",
      "299      0              what do i need to enter the programme\n",
      "300      0     what do i need to enter the computer programme\n",
      "301      0  what do i need to improve my book of job skill...\n",
      "302      0              what do i need to enter the broadcast\n",
      "303      0        what do i need to enter the figurer program\n",
      "304      0             what do i need to get in the programme\n",
      "305      0     what do i need to go in the computer programme\n",
      "306      0     what do i need to go into the computer program\n",
      "307      0  what do i need to join the list of grade at th...\n",
      "308      0  what do i need to go in the computer course of...\n",
      "309      0     what do i need to go in the computer programme\n",
      "310      0  what do i need to do to enter the computer pro...\n",
      "311      0  what do i need to enter the computer computer ...,      label                                           sentence\n",
      "312      0       what do i need to enter the computer program\n",
      "313      0              what do i postulate to enter the exam\n",
      "314      0         what do i postulate to recruit the program\n",
      "315      0  what do i postulate to enter the computer prog...\n",
      "316      0            what do i need to recruit the programme\n",
      "317      0              what do i need to scram an internship\n",
      "318      0             what do i need to scram in the program\n",
      "319      0          what do i need to do to enter the program\n",
      "320      0          what do i need to bring in the internship\n",
      "321      0        what do i need to bring forth an internship\n",
      "322      0     what do i need to go in the computer programme\n",
      "323      0  what do i need to get in the details of course...\n",
      "324      0             what do i need to get in the programme\n",
      "325      0          what do i need to do to get an internship\n",
      "326      0             what do i need to get in the programme\n",
      "327      0              what do i need to get in the platform\n",
      "328      0            what do i need to recruit the programme\n",
      "329      0             what do i need to fetch in the program\n",
      "330      0             what do i need to fetch in the program\n",
      "331      0    what do i indigence to bugger off an internship\n",
      "332      0          what do i indigence to get in the program\n",
      "333      0             what do i indigence to get in the exam\n",
      "334      0    what do i need to bow in the computer programme\n",
      "335      0     what do i need to enter the computer programme,      label                                           sentence\n",
      "336      0  what do i need to enter the data processor pro...\n",
      "337      0     what do i need to go in the computer programme\n",
      "338      0       what do i need to go in the computer program\n",
      "339      0     what do i need to go in the computer programme\n",
      "340      0              what do i need to start an internship\n",
      "341      0     what do i need to go in the computer programme\n",
      "342      0     what do i need to go into the computer program\n",
      "343      0              what do i need to recruit the program\n",
      "344      0      what do i need to record the term of the test\n",
      "345      0    what do i need to record the computer programme\n",
      "346      0   what do i need to go into the computer programme\n",
      "347      0     what do i need to enter the computer programme\n",
      "348      0     what do i need to enter the computer programme\n",
      "349      0  what do i involve to enter the computer programme\n",
      "350      0             what do i involve to enter the program\n",
      "351      0    what do i involve to enter the computer program\n",
      "352      0             what do i need to get in the programme\n",
      "353      0            what do i need to introduce the program\n",
      "354      0          what do i need to introduce the broadcast\n",
      "355      0    what do i need to get in the computer programme\n",
      "356      0     what do i need to enter the computer programme\n",
      "357      0                what do i need to enter the program\n",
      "358      0                what do i need to adjust internship\n",
      "359      0       what do i need to participate in the program,      label                                           sentence\n",
      "360      0        what do i need to participate the broadcast\n",
      "361      0  what do i motive to do to bugger off an intern...\n",
      "362      0  what do i motive to enter the interrogation co...\n",
      "363      0            what do i motive to enter the broadcast\n",
      "364      0   what do i need to take in the computer programme\n",
      "365      0            what do i need to recruit the programme\n",
      "366      0            what do i need to recruit the broadcast\n",
      "367      0      what do i postulate to recruit the posit exam\n",
      "368      0  what do i postulate to enter the computer program\n",
      "369      0  what do i postulate to enter the computer proc...\n",
      "370      1                    how can i engender scholarships\n",
      "371      1                    how can i engender scholarships\n",
      "372      1                    how can i engender scholarships\n",
      "373      1                    how can i engender scholarships\n",
      "374      1                       how can i beget scholarships\n",
      "375      1                       how can i beget scholarships\n",
      "376      1                       how can i beget scholarships\n",
      "377      1                       how can i beget scholarships\n",
      "378      1  how can i generate <SEP>how can i generate tax...\n",
      "379      1  how can i generate <SEP>how can i generate sch...\n",
      "380      1  how can i generate <SEP>how can i generate doc...\n",
      "381      1  how can i generate <SEP>how can i generate tex...\n",
      "382      1  how can i bring forth documents for admission fee\n",
      "383      1            how can i bring up separate examination,      label                                       sentence\n",
      "384      1  how can i bring together documents for entree\n",
      "385      1     how can i bring forth documents for entree\n",
      "386      1                     how can i get scholarships\n",
      "387      1                  how can i get in scholarships\n",
      "388      1                       how can i get a capriole\n",
      "389      1                     how can i get scholarships\n",
      "390      1      how <SEP>how much do i ask to spawn a job\n",
      "391      1  how <SEP>how can i submit enrolment text file\n",
      "392      1           how <SEP>how can i enroll a platform\n",
      "393      1             how <SEP>how do i submit my papers\n",
      "394      1                how can i generate scholarships\n",
      "395      1                    how can i generate workbook\n",
      "396      1                    how can i generate learning\n",
      "397      1                    how can i generate learning\n",
      "398      1   how can i generate document for scholarships\n",
      "399      1               how can i generate encyclopedism\n",
      "400      1                 how can i generate scholarship\n",
      "401      1                 how can i generate scholarship\n",
      "402      1   how can i generate document for scholarships\n",
      "403      1               how can i generate encyclopedism\n",
      "404      1               how can i generate encyclopedism\n",
      "405      1                how can i engender scholarships\n",
      "406      1                how can i engender scholarships\n",
      "407      1                how can i engender scholarships,      label                                           sentence\n",
      "408      1                    how can i engender scholarships\n",
      "409      1                   how <SEP>how do i get an inquiry\n",
      "410      1                how <SEP>how can i get scholarships\n",
      "411      1  how <SEP>how can i put forward documents for e...\n",
      "412      1  how <SEP>how many clubs are presently operatin...\n",
      "413      1                   how can i generate encyclopedism\n",
      "414      1                how can i generate lesson on campus\n",
      "415      1                        how can i generate learning\n",
      "416      1                        how can i generate learning\n",
      "417      1  how can i get in track of course of action whe...\n",
      "418      1                         how can i get scholarships\n",
      "419      1          how can i get a line of work when i study\n",
      "420      1                      how can i get in scholarships\n",
      "421      1                         how can i get scholarships\n",
      "422      1        how can i get documents for admission price\n",
      "423      1  how can i get a line topic of scientific research\n",
      "424      1                         how can i get scholarships\n",
      "425      1  how <SEP>how a lot does it cost to live in dor...\n",
      "426      1             how <SEP>how do i present scholarships\n",
      "427      1               how <SEP>how test will be comparable\n",
      "428      1                     how <SEP>how exam is organised\n",
      "429      1          how can i find out topics of dissertation\n",
      "430      1                       how can i find encyclopedism\n",
      "431      1                       how can i find encyclopedism,      label                                           sentence\n",
      "432      1  how can i find topics of dissertation at the p...\n",
      "433      1                       how can i find encyclopedism\n",
      "434      1                       how can i find encyclopedism\n",
      "435      1             how can i find a topic of dissertation\n",
      "436      1          how can i find documents for scholarships\n",
      "437      1            how can i find part clock in university\n",
      "438      1  how can i find the list of grade at this polit...\n",
      "439      1             how can i catch a job during my canvas\n",
      "440      1               how can i catch erudition in college\n",
      "441      1              how can i catch erudition in the exam\n",
      "442      1  how can i obtain scholarships to international...\n",
      "443      1                      how can i obtain scholarships\n",
      "444      1                         how can i obtain erudition\n",
      "445      1                         how can i obtain erudition\n",
      "446      1                            how can i fix erudition\n",
      "447      1                            how can i fix erudition\n",
      "448      1                            how can i fix erudition\n",
      "449      1                            how can i fix erudition\n",
      "450      1                           how can i beat erudition\n",
      "451      1              how can i beat erudition without exam\n",
      "452      1                 how can i beat erudition on campus\n",
      "453      1                      how can i beat erudition test\n",
      "454      1                 how can i get documents for entree\n",
      "455      1                          how can i get eruditeness,      label                                    sentence\n",
      "456      1                   how can i get eruditeness\n",
      "457      1             how can i get hold of documents\n",
      "458      1               how can i get in line of work\n",
      "459      1        how can i get down erudition details\n",
      "460      1                how can i get down erudition\n",
      "461      1                how can i have encyclopedism\n",
      "462      1                how can i have encyclopedism\n",
      "463      1                how can i have encyclopedism\n",
      "464      1         how can i get in persistently a job\n",
      "465      1                  how can i get scholarships\n",
      "466      1                   how can i get scholarship\n",
      "467      1                   how can i get scholarship\n",
      "468      1  how can i get a line topic of dissertation\n",
      "469      1                   how can i get eruditeness\n",
      "470      1               how can i get eruditeness tax\n",
      "471      1          how can i buzz off topic of thesis\n",
      "472      1            how can i buzz off encyclopedism\n",
      "473      1            how can i buzz off encyclopedism\n",
      "474      1               how can i cause encyclopedism\n",
      "475      1               how can i cause encyclopedism\n",
      "476      1               how can i cause encyclopedism\n",
      "477      1                  how can i get scholarships\n",
      "478      1                how can i get encyclopaedism\n",
      "479      1                how can i get encyclopaedism,      label                                           sentence\n",
      "480      1  how can i muddle scholarships <SEP>how can i m...\n",
      "481      1  how can i muddle scholarships <SEP>how can i m...\n",
      "482      1  how can i muddle scholarships <SEP>how can i m...\n",
      "483      1  how can i muddle scholarships <SEP>how can i m...\n",
      "484      1                     how <SEP>how test will be like\n",
      "485      1  how <SEP>how do i render documents in english ...\n",
      "486      1               how <SEP>how test will be comparable\n",
      "487      1    how <SEP>how do i submit registration documents\n",
      "488      1                        how can i fix encyclopedism\n",
      "489      1                        how can i fix encyclopedism\n",
      "490      1                        how can i fix encyclopedism\n",
      "491      1                             how can i fix learning\n",
      "492      1                             how can i fix learning\n",
      "493      1  how can i fix learning without extraneous lear...\n",
      "494      1                             how can i fix learning\n",
      "495      1  how <SEP>how many clubs are currently head in ...\n",
      "496      1           how <SEP>how can i generate scholarships\n",
      "497      1                how <SEP>how test will be the likes\n",
      "498      1                    how <SEP>how exam are organized\n",
      "499      1                         how can i get scholarships\n",
      "500      1             how can i get along club in university\n",
      "501      1                         how can i get scholarships\n",
      "502      1                      how can i get in scholarships\n",
      "503      1  does the program offer any scholarships to int...,      label                                           sentence\n",
      "504      1  does the program crack any scholarships to int...\n",
      "505      1  does the program crack any scholarships to int...\n",
      "506      1  does the program offer any scholarships to int...\n",
      "507      1  does the program put forth any scholarships to...\n",
      "508      1  does the program put up any scholarships to in...\n",
      "509      1  does the program offer any learning to interna...\n",
      "510      1  does the program pass any scholarships to inte...\n",
      "511      1  does the program pass any scholarships to inte...\n",
      "512      1  does the political platform take hard nosed tasks\n",
      "513      1  does the political platform offer any scholars...\n",
      "514      1  does the political platform offer any scholars...\n",
      "515      1  does the political platform offer any scholars...\n",
      "516      1  does the political platform offer any learning...\n",
      "517      1  does the political platform offer any scholars...\n",
      "518      1  does the political platform offer any scholars...\n",
      "519      1  does the political platform offer any scholars...\n",
      "520      1  does the political platform offer any scholars...\n",
      "521      1  does the program fling any scholarships to int...\n",
      "522      1  does the program offer any scholarships to out...\n",
      "523      1  does the program offer any learning to interna...\n",
      "524      1  does the program offer any learnedness to inte...\n",
      "525      1  does the program offer any scholarships to int...\n",
      "526      1  does the program offer any learnedness to inte...\n",
      "527      1  does the programme offer any scholarship to in...,      label                                           sentence\n",
      "528      1  does the programme offer any scholarships to i...\n",
      "529      1  does the programme offer any scholarship to in...\n",
      "530      1  does the program fling any scholarships to int...\n",
      "531      1  does the program offer any scholarships to out...\n",
      "532      1  does the program offer any scholarship to outs...\n",
      "533      1  does the program offer any scholarships to int...\n",
      "534      1  does the program offer any scholarships to int...\n",
      "535      1  does the program offer any scholarship to inte...\n",
      "536      1  does the program offer any scholarship to outs...\n",
      "537      1  does the program offer any scholarship to inte...\n",
      "538      1  does the program offer up any scholarship to i...\n",
      "539      1  does the program fling any scholarships to int...\n",
      "540      1  does the program fling any scholarships to int...\n",
      "541      1  does the program fling any scholarships to int...\n",
      "542      1  does the program offer any scholarships to int...\n",
      "543      1  does the program offer any scholarships to int...\n",
      "544      1         does the program discard eruditeness tests\n",
      "545      1  does the program discard document for entrance...\n",
      "546      1  does the computer programme offer any learnedn...\n",
      "547      1  does the computer programme offer any scholars...\n",
      "548      1  does the computer programme fling any scholars...\n",
      "549      1  does the program give any scholarships to inte...\n",
      "550      1  does the program fling any scholarships to out...\n",
      "551      1  does the program fling any scholarships to int...,      label                                           sentence\n",
      "552      1  does the program offer any scholarships to int...\n",
      "553      1  does the program fling any scholarships to int...\n",
      "554      1  does the program fling any scholarships to int...\n",
      "555      1        does the curriculum call for any particular\n",
      "556      1  does the curriculum fling any scholarships to ...\n",
      "557      1  does the curriculum fling any scholarships to ...\n",
      "558      1  does the platform offer any scholarships to in...\n",
      "559      1  does the platform offer any scholarships to in...\n",
      "560      1  does the platform offer any scholarships to in...\n",
      "561      1  does the program offer any scholarship to inte...\n",
      "562      1  does the program offer any scholarship to inte...\n",
      "563      1  does the program offer any scholarships to int...\n",
      "564      1  does the program offer any scholarships to int...\n",
      "565      1  does the program offer any scholarship to inte...\n",
      "566      1  does the program offer any encyclopaedism to i...\n",
      "567      1  does the program fling forth any scholarships ...\n",
      "568      1  does the program offering any research to inte...\n",
      "569      1  does the program offering any scholarships to ...\n",
      "570      1  does the program offer any scholarships to int...\n",
      "571      1  does the program proffer any scholarship to in...\n",
      "572      1  does the program proffer any scholarships to i...\n",
      "573      1  does the program offer any scholarships to int...\n",
      "574      1  does the program offer any scholarships to int...\n",
      "575      1  does the program offer any scholarships to int...,      label                                           sentence\n",
      "576      1  does the program offer any scholarship to outs...\n",
      "577      1  does the program fling any scholarships to out...\n",
      "578      1  does the program volunteer any scholarships to...\n",
      "579      1  does the program volunteer watch any watchable...\n",
      "580      1        does the political program teach in english\n",
      "581      1  does the political program fling any scholarsh...\n",
      "582      1  does the political program offer any scholarsh...\n",
      "583      1  does the political program offer any learning ...\n",
      "584      1  does the computer programme syllabus advise i ...\n",
      "585      1  does the computer programme offer any scholars...\n",
      "586      1  does the computer programme offer any scholars...\n",
      "587      1  does the program offer any scholarships to int...\n",
      "588      1  does the program offer any learnedness to inte...\n",
      "589      1  does the program offer any scholarships to out...\n",
      "590      1  does the program offer any scholarships to int...\n",
      "591      1  does the program offer any scholarships to int...\n",
      "592      1  does the program offer any encyclopedism to in...\n",
      "593      1  does the program offer any scholarships to int...\n",
      "594      1  does the program offer any scholarships to int...\n",
      "595      1  does the program offer any scholarships to out...\n",
      "596      1  does the program offer any scholarship to inte...\n",
      "597      1  does the program crack any scholarships to int...\n",
      "598      1  does the program crack any scholarships to int...\n",
      "599      1  does the program offer any learning to interna...,      label                                           sentence\n",
      "600      1  does the program offer any scholarships to int...\n",
      "601      1  does the program offer up any scholarships to ...\n",
      "602      1  does the program psychometric test any scholar...\n",
      "603      1  does the program fling any scholarships to int...\n",
      "604      1  does the program fling any scholarships to int...\n",
      "605      1  does the program offer any breakthrough learni...\n",
      "606      1  does the program provide any scholarships to i...\n",
      "607      1  does the program provide any scholarships to i...\n",
      "608      1  does the program aim to educate the educatee i...\n",
      "609      1  does the program offer any scholarships to out...\n",
      "610      1  does the program offer any learnedness to inte...\n",
      "611      1  does the program offer any learnedness to inte...\n",
      "612      1  does the program offer any scholarships to out...\n",
      "613      1  does the program offer any scholarships to int...\n",
      "614      1  is there scholarships to international applicator\n",
      "615      1  is there scholarships to international applicator\n",
      "616      1  is there scholarships to international applicator\n",
      "617      1  is there scholarships to international applicator\n",
      "618      1         is there learning of grade at this program\n",
      "619      1         is there learning to international applier\n",
      "620      1         is there learning to international applier\n",
      "621      1   is there scholarships to international applicant\n",
      "622      1   is there scholarships to international applicant\n",
      "623      1   is there scholarships to international applicant,      label                                           sentence\n",
      "624      1   is there scholarships to international applicant\n",
      "625      1      is there scholarship to international applier\n",
      "626      1      is there scholarship to international applier\n",
      "627      1      is there scholarship to international applier\n",
      "628      1      is there scholarship to international applier\n",
      "629      1   is there encyclopaedism to international applier\n",
      "630      1   is there encyclopaedism to international applier\n",
      "631      1   is there encyclopaedism to international applier\n",
      "632      1      is there learning to international applicants\n",
      "633      1         is there learning to international applier\n",
      "634      1         is there learning to international applier\n",
      "635      1        is there scholarships to outside applicants\n",
      "636      1           is there scholarships to outside applier\n",
      "637      1           is there scholarships to outside applier\n",
      "638      1           is there scholarships to outside applier\n",
      "639      1           is there scholarships to outside applier\n",
      "640      1        is there scholarships to outside applicants\n",
      "641      1           is there scholarships to outside applier\n",
      "642      1           is there scholarships to outside applier\n",
      "643      1      is there eruditeness to international applier\n",
      "644      1         is there eruditeness to outside applicants\n",
      "645      1         is there eruditeness to outside applicants\n",
      "646      1  is <SEP>is it potential enhancement to study i...\n",
      "647      1                        is <SEP>is there internship,      label                                           sentence\n",
      "648      1  is <SEP>is there anyone who can support me whe...\n",
      "649      1  is <SEP>is it acceptable to perplex a job when...\n",
      "650      1   is there eruditeness to international applicants\n",
      "651      1         is there eruditeness to outside applicants\n",
      "652      1         is there eruditeness to outside applicants\n",
      "653      1           is there scholarships to outside applier\n",
      "654      1        is there scholarships to outside applicants\n",
      "655      1         is there scholarships to outside applicant\n",
      "656      1         is there scholarships to outside applicant\n",
      "657      1  is there scholarships <SEP>is there scholarshi...\n",
      "658      1  is there scholarships <SEP>is there scholarshi...\n",
      "659      1  is there scholarships <SEP>is there scholarshi...\n",
      "660      1  is there scholarships <SEP>is there scholarshi...\n",
      "661      1          is there scholarships to external applier\n",
      "662      1          is there scholarships to external applier\n",
      "663      1          is there scholarships to external applier\n",
      "664      1          is there scholarships to external applier\n",
      "665      1  is there scholarships to extraneous students i...\n",
      "666      1        is there scholarships to extraneous applier\n",
      "667      1     is there scholarships to extraneous applicants\n",
      "668      1  is there scholarships to extraneous students i...\n",
      "669      1        is there scholarships to external applicant\n",
      "670      1        is there scholarships to external applicant\n",
      "671      1        is there scholarships to external applicant,      label                                           sentence\n",
      "672      1        is there scholarships to external applicant\n",
      "673      1          is there scholarships to external applier\n",
      "674      1          is there scholarships to external applier\n",
      "675      1          is there scholarships to external applier\n",
      "676      1          is there scholarships to external applier\n",
      "677      1                      is there learning in the exam\n",
      "678      1           is there learning to external applicants\n",
      "679      1           is there learning to external applicants\n",
      "680      1           is there learning to external applicants\n",
      "681      1   is there encyclopaedism to international applier\n",
      "682      1   is there encyclopaedism to international applier\n",
      "683      1  is there encyclopaedism to international appli...\n",
      "684      1   is there encyclopaedism to international applier\n",
      "685      1      is there encyclopaedism to outside applicants\n",
      "686      1      is there encyclopaedism to outside applicants\n",
      "687      1  is there <SEP>is there encyclopaedism to inter...\n",
      "688      1  is there <SEP>is there anyone who can assistan...\n",
      "689      1  is there <SEP>is there anyone who can help me ...\n",
      "690      1  is there <SEP>is there hard nosed tasks in the...\n",
      "691      1   is there encyclopaedism to international applier\n",
      "692      1     is there encyclopaedism to external applicants\n",
      "693      1     is there encyclopaedism to external applicants\n",
      "694      1  is there encyclopedism to international applicant\n",
      "695      1    is there encyclopedism to international applier,      label                                           sentence\n",
      "696      1  is there encyclopedism to international applic...\n",
      "697      1   is there encyclopaedism to international applier\n",
      "698      1     is there encyclopaedism to external applicants\n",
      "699      1     is there encyclopaedism to external applicants\n",
      "700      1    is there encyclopedism to international applier\n",
      "701      1    is there encyclopedism to international applier\n",
      "702      1  is there encyclopedism to international applicant\n",
      "703      1     is there scholarships to international applier\n",
      "704      1     is there scholarships to international applier\n",
      "705      1     is there scholarships to international applier\n",
      "706      1     is there scholarships to international applier\n",
      "707      1     is there scholarships to international applier\n",
      "708      1     is there scholarships to international applier\n",
      "709      1     is there scholarships to international applier\n",
      "710      1     is there scholarships to international applier\n",
      "711      1    is there scholarship to international applicant\n",
      "712      1    is there scholarship to international applicant\n",
      "713      1    is there scholarship to international applicant\n",
      "714      1    is there scholarship to international applicant\n",
      "715      1     is there scholarships to international applier\n",
      "716      1     is there scholarships to international applier\n",
      "717      1     is there scholarships to international applier\n",
      "718      1     is there scholarships to international applier\n",
      "719      1           is there scholarships to outside applier,      label                                           sentence\n",
      "720      1           is there scholarships to outside applier\n",
      "721      1         is there scholarships to outside applicant\n",
      "722      1         is there scholarships to outside applicant\n",
      "723      1    is there learnedness to international applicant\n",
      "724      1      is there learnedness to international applier\n",
      "725      1      is there learnedness to international applier\n",
      "726      1      is there learnedness to international applier\n",
      "727      1   is there learnedness to international applicants\n",
      "728      1    is there learnedness to international applicant\n",
      "729      1    is there learnedness to international applicant\n",
      "730      1   is there encyclopaedism to international applier\n",
      "731      1   is there encyclopaedism to international applier\n",
      "732      1  is there encyclopaedism to international appli...\n",
      "733      1    is there learnedness to international applicant\n",
      "734      1   is there learnedness to international applicants\n",
      "735      1      is there learnedness to international applier\n",
      "736      1    is there learnedness to international applicant\n",
      "737      1   is there learnedness to international applicants\n",
      "738      1      is there learnedness to international applier\n",
      "739      1   is there learnedness to international applicants\n",
      "740      1      is there learnedness to international applier\n",
      "741      1    is there learnedness to international applicant\n",
      "742      1      is there learnedness to international applier\n",
      "743      1      is there learnedness to international applier,      label                                           sentence\n",
      "744      2  who will help me when i run short to russian c...\n",
      "745      2              who will help me when i rifle to ussr\n",
      "746      2     who will help me when i rifle to soviet russia\n",
      "747      2  who will avail me when i lead eruditeness aim ...\n",
      "748      2  who will avail me when i go to russian confede...\n",
      "749      2       who will avail me when i go to soviet russia\n",
      "750      2    who will assistant me when i get in the carrizo\n",
      "751      2  who will assistant me when i go to russian con...\n",
      "752      2   who will assistant me when i go to soviet russia\n",
      "753      2           who will aid me when i drive in the exam\n",
      "754      2  who will aid me when i go to soviet federated ...\n",
      "755      2         who will aid me when i go to soviet russia\n",
      "756      2  who will supporter me when i run short to russ...\n",
      "757      2  who will supporter me when i go to russian con...\n",
      "758      2   who will supporter me when i go to soviet russia\n",
      "759      2        who will help me when i run short to russia\n",
      "760      2  who will help me when i plump to russian sovie...\n",
      "761      2     who will help me when i plump to soviet russia\n",
      "762      2         who will help me when i go to soviet union\n",
      "763      2  who will help me when i go to russian soviet f...\n",
      "764      2  who will help me when i go to russian soviet f...\n",
      "765      2  who will help me when i get up to russian sovi...\n",
      "766      2  who will help oneself me when i run short to ussr\n",
      "767      2  who will help oneself me when i go to soviet r...,      label                                           sentence\n",
      "768      2        who will help me when i go to soviet russia\n",
      "769      2  who will help me when i go to russian soviet f...\n",
      "770      2  who will help me when i go to russian confederacy\n",
      "771      2        who will assist me when i come to know mode\n",
      "772      2                   who will assist me when i go far\n",
      "773      2  who will assist me when i go to russian confed...\n",
      "774      2        who will help me when i go to soviet russia\n",
      "775      2  who will help me when i sound to russian feder...\n",
      "776      2  who will help me when i sound to russian confe...\n",
      "777      2     who will help me when i need to enter the exam\n",
      "778      2             who will help me when i endure to ussr\n",
      "779      2  who will help me when i endure to russian conf...\n",
      "780      2                           who will serve as escort\n",
      "781      2  who will serve me when i run short to russian ...\n",
      "782      2       who will serve me when i run short to russia\n",
      "783      2       who will help me when i run out in nightfall\n",
      "784      2          who will help me when i run short to ussr\n",
      "785      2  who will help me when i run short to union sov...\n",
      "786      2  who will help me when i go to russian confederacy\n",
      "787      2          who will help me when i run short to ussr\n",
      "788      2          who will help me when i run short to ussr\n",
      "789      2        who will help me when i go to soviet russia\n",
      "790      2        who will help me when i run short to russia\n",
      "791      2  who will help me when i run brusque number 2 w...,      label                                           sentence\n",
      "792      2       who will serve me when i run short to russia\n",
      "793      2   who will serve me when i run short in university\n",
      "794      2       who will serve me when i run short to russia\n",
      "795      2  who will assistance me when i go to soviet russia\n",
      "796      2    who will assistance me when i run short to ussr\n",
      "797      2  who will assistance me when i run short to russia\n",
      "798      2      who will assist me when i go to soviet russia\n",
      "799      2      who will assist me when i go to soviet russia\n",
      "800      2  who will assist me when i go to russian federa...\n",
      "801      2  who will assist me when i run short to russian...\n",
      "802      2  who will assist me when i go to the political ...\n",
      "803      2      who will assist me when i go to soviet russia\n",
      "804      2            who will attend the watch out scheduled\n",
      "805      2  who will attend to me when i go to russian con...\n",
      "806      2          who will attend to me when i go to russia\n",
      "807      2  who will assist me when i go to russian soviet...\n",
      "808      2           who will assist me when i live on campus\n",
      "809      2        who will assist me when i live on to russia\n",
      "810      2                   who will assist me when i get in\n",
      "811      2      who will assist me when i go to soviet russia\n",
      "812      2              who will assist me when i go to union\n",
      "813      2                who will serve as escort when i get\n",
      "814      2       who will serve me when i go to soviet russia\n",
      "815      2              who will serve me when i go to russia,      label                                           sentence\n",
      "816      2          who will help me when i run short to ussr\n",
      "817      2                     who will help me when i fit in\n",
      "818      2                who will help me when i fit to ussr\n",
      "819      2  who will help me when i run short to russian s...\n",
      "820      2   who will help me when i proceed to soviet russia\n",
      "821      2            who will help me when i proceed to ussr\n",
      "822      2  who will help me when i go to russian confederacy\n",
      "823      2                     who will help me when i go far\n",
      "824      2         who will help me when i go to soviet union\n",
      "825      2  who will help me when i go to russian confederacy\n",
      "826      2                     who will help me when i go far\n",
      "827      2               who will help me when i go to russia\n",
      "828      2   who will help me when i go to russian federation\n",
      "829      2           who will help me when i hold out to ussr\n",
      "830      2           who will help me when i hold out to ussr\n",
      "831      2  who will help me when i run short to ussr or o...\n",
      "832      2       who will help me when i go by bank of thesis\n",
      "833      2               who will help me when i go to russia\n",
      "834      2        who will help me when i go to soviet russia\n",
      "835      2  who will help me when i run short to russian f...\n",
      "836      2  who will help me when i run short to russian s...\n",
      "837      2          who will help me when i run short to ussr\n",
      "838      2        who will help me when i go to soviet russia\n",
      "839      2  who will help me when i go to russian confederacy,      label                                           sentence\n",
      "840      2      who will helper me when i get to soviet union\n",
      "841      2  who will helper me when i go through the exami...\n",
      "842      2  who will helper me when i go to russian soviet...\n",
      "843      2  who will help me when i run short to russian c...\n",
      "844      2  who will help me when i go to russian confederacy\n",
      "845      2  who will help me when i go to russian confederacy\n",
      "846      2  who will help me when i get down to the comput...\n",
      "847      2        who will help me when i go to soviet russia\n",
      "848      2  who will help me when i go to russian confederacy\n",
      "849      2  who will help me when i get down to 0 who will...\n",
      "850      2  who will help me when i go through to russian ...\n",
      "851      2  who will help me when i go to russian confederacy\n",
      "852      2  it is unmanageable for me in the first few day...\n",
      "853      2  it is unmanageable for me in the first few mea...\n",
      "854      2  it is unmanageable for me in the maiden few da...\n",
      "855      2  it is unmanageable for me in the maiden few da...\n",
      "856      2  it is difficult for me in the first few days w...\n",
      "857      2  it is difficult for me in the first few days w...\n",
      "858      2  it is difficult for me in the first few days w...\n",
      "859      2  it is difficult for me in the first few days w...\n",
      "860      2  it is unmanageable for me in the first few day...\n",
      "861      2  it is unmanageable for me in the get go few da...\n",
      "862      2  it is unmanageable for me in the get go few da...\n",
      "863      2  it is unwieldy for me in the first few days wh...,      label                                           sentence\n",
      "864      2  it is unwieldy for me in the first few days wh...\n",
      "865      2  it is unwieldy for me in the first few days wh...\n",
      "866      2  it is unwieldy for me in the first few days wh...\n",
      "867      2  it is unmanageable for me in the first few mea...\n",
      "868      2  it is unmanageable for me in the first few mea...\n",
      "869      2  it is unmanageable for me in the first few day...\n",
      "870      2  it is unmanageable for me in the first few day...\n",
      "871      2  it is unmanageable for me in the maiden few da...\n",
      "872      2  it is unmanageable for me in the first few mea...\n",
      "873      2  it is unmanageable for me in the first few day...\n",
      "874      2  it is unmanageable for me in the first few day...\n",
      "875      2  it is hard for me in the first few clarence da...\n",
      "876      2  it is hard for me in the first few clarence da...\n",
      "877      2  it is hard for me in the first few clarence da...\n",
      "878      2  it is hard for me in the first few clarence da...\n",
      "879      2  it is hard for me in the first few mean solar ...\n",
      "880      2  it is hard for me in the first few mean solar ...\n",
      "881      2  it is hard for me in the first few mean solar ...\n",
      "882      2  it is hard for me in the first few mean solar ...\n",
      "883      2  it is hard for me in the first few sidereal da...\n",
      "884      2  it is hard for me in the first few sidereal da...\n",
      "885      2  it is hard for me in the first few sidereal da...\n",
      "886      2  it is hard for me in the outset few years when...\n",
      "887      2  it is hard for me in the outset few days when ...,      label                                           sentence\n",
      "888      2  it is hard for me in the outset few days when ...\n",
      "889      2  it is hard for me in the outset few days when ...\n",
      "890      2  it is operose for me in the first few mean sol...\n",
      "891      2  it is operose for me in the first few mean sol...\n",
      "892      2  it is operose for me in the first few days whe...\n",
      "893      2  it is operose for me in the first few days whe...\n",
      "894      2  it is hard for me in the first few days when i...\n",
      "895      2  it is hard for me in the first few days when i...\n",
      "896      2  it is hard for me in the first few days when i...\n",
      "897      2  it is hard for me in the first few days when i...\n",
      "898      2  it is difficult for me in the first few claren...\n",
      "899      2  it is difficult for me in the first few claren...\n",
      "900      2  it is difficult for me in the first few claren...\n",
      "901      2  it is difficult for me in the first few claren...\n",
      "902      2  it is difficult for me in the first few day wh...\n",
      "903      2  it is difficult for me in the first few day wh...\n",
      "904      2  it is difficult for me in the first few day wh...\n",
      "905      2  it is difficult for me in the first few day wh...\n",
      "906      2  it is difficult for me in the first few day wh...\n",
      "907      2  it is difficult for me in the first few day wh...\n",
      "908      2  it is difficult for me in the first few day wh...\n",
      "909      2  it is difficult for me in the first few day wh...\n",
      "910      2  it is unmanageable for me in the first few mea...\n",
      "911      2  it is unmanageable for me in the first few day...,      label                                           sentence\n",
      "912      2  it is unmanageable for me in the first few day...\n",
      "913      2  it is hard for me in the first few day when i ...\n",
      "914      2  it is hard for me in the first few day when i get\n",
      "915      2  it is hard for me in the first few day when i ...\n",
      "916      2  it is hard for me in the first few day when i ...\n",
      "917      2  it is difficult for me in the first off few da...\n",
      "918      2  it is difficult for me in the first off few da...\n",
      "919      2  it is difficult for me in the first off few da...\n",
      "920      2  it is difficult for me in the first off few da...\n",
      "921      2  it is difficult for me in the first few years ...\n",
      "922      2  it is difficult for me in the first few years ...\n",
      "923      2  it is difficult for me in the first few years ...\n",
      "924      2  it is difficult for me in the first few years ...\n",
      "925      2  it is difficult for me in the offset of course...\n",
      "926      2  it is difficult for me in the offset few days ...\n",
      "927      2  it is difficult for me in the offset few days ...\n",
      "928      2  it is difficult for me in the offset few days ...\n",
      "929      2  it is difficult for me in the first few days w...\n",
      "930      2  it is difficult for me in the first few days w...\n",
      "931      2  it is difficult for me in the first few days w...\n",
      "932      2  it is difficult for me in the first few days w...\n",
      "933      2  it is difficult for me in the first few daylig...\n",
      "934      2  it is difficult for me in the first few daylig...\n",
      "935      2  it is difficult for me in the first few daylig...,      label                                           sentence\n",
      "936      2  it is difficult for me in the first few daylig...\n",
      "937      2  it is difficult for me in the first few hour w...\n",
      "938      2  it is difficult for me in the first few hour i...\n",
      "939      2  it is difficult for me in the first few hour i...\n",
      "940      2  it is difficult for me in the first of all few...\n",
      "941      2  it is difficult for me in the first of all few...\n",
      "942      2  it is difficult for me in the first of all few...\n",
      "943      2  it is difficult for me in the first of all few...\n",
      "944      2  it is unmanageable for me in the start up few ...\n",
      "945      2  it is unmanageable for me in the first few mea...\n",
      "946      2  it is unmanageable for me in the first few mea...\n",
      "947      2  it is difficult for me in the number variety o...\n",
      "948      2  it is difficult for me in the number few mean ...\n",
      "949      2  it is difficult for me in the number few mean ...\n",
      "950      2  it is hard for me in the first few mean solar ...\n",
      "951      2  it is hard for me in the first few mean solar ...\n",
      "952      2  it is hard for me in the first few mean solar ...\n",
      "953      2  it is hard for me in the first few mean solar ...\n",
      "954      2  it is difficult for me in the first few mean s...\n",
      "955      2  it is difficult for me in the first few mean s...\n",
      "956      2  it is difficult for me in the first few mean s...\n",
      "957      2  it is difficult for me in the first few mean s...\n",
      "958      2  it is difficult for me in the first few imply ...\n",
      "959      2  it is difficult for me in the first few imply ...,      label                                           sentence\n",
      "960      2  it is difficult for me in the first few imply ...\n",
      "961      2  it is hard for me in the first few mean solar ...\n",
      "962      2  it is hard for me in the first few mean solar ...\n",
      "963      2  it is hard for me in the first few mean solar ...\n",
      "964      2  it is hard for me in the first few mean solar ...\n",
      "965      2  it is difficult for me in the outset few days ...\n",
      "966      2  it is difficult for me in the outset few days ...\n",
      "967      2  it is difficult for me in the outset few days ...\n",
      "968      2  it is difficult for me in the outset few days ...\n",
      "969      2  it is hard for me in the showtime few days whe...\n",
      "970      2  it is hard for me in the showtime few sidereal...\n",
      "971      2  it is hard for me in the showtime few days whe...\n",
      "972      2  it is difficult for me in the showtime few sid...\n",
      "973      2  it is difficult for me in the showtime few sid...\n",
      "974      2  it is difficult for me in the showtime few twe...\n",
      "975      2  it is difficult for me in the showtime few sid...\n",
      "976      2  it is difficult for me in the showtime few sid...\n",
      "977      2  it is difficult for me in the showtime few sid...\n",
      "978      2  it is difficult for me in the outset few mean ...\n",
      "979      2  it is difficult for me in the outset few mean ...\n",
      "980      2  it is difficult for me in the outset few days ...\n",
      "981      2  it is difficult for me in the outset few days ...\n",
      "982      2  it is difficult for me in the showtime few sid...\n",
      "983      2  it is difficult for me in the showtime few sid...,       label                                           sentence\n",
      "984       2  it is difficult for me in the showtime few twe...\n",
      "985       2          who will be my batchmates at the syllabus\n",
      "986       2  who will be my batchmates at the calculator pr...\n",
      "987       2  who will be my batchmates at the political pol...\n",
      "988       2  who will be my batchmates at the political pol...\n",
      "989       2  who will be my batchmates at the course of the...\n",
      "990       2          who will be my batchmates at the syllabus\n",
      "991       2  who will be my batchmates at the political wea...\n",
      "992       2  who will be my batchmates at the political wea...\n",
      "993       2         who will be my batchmates at the programme\n",
      "994       2  who will be my batchmates at the political cho...\n",
      "995       2  who will be my batchmates at the political cho...\n",
      "996       2  who will be my batchmates at the course of stu...\n",
      "997       2  who will be my batchmates at the political pla...\n",
      "998       2  who will be my batchmates at the political pol...\n",
      "999       2  who will be my batchmates at the political pol...\n",
      "1000      2              who will be my batchmates at the exam\n",
      "1001      2  who will be my batchmates at the political cho...\n",
      "1002      2  who will be my batchmates at the political cho...\n",
      "1003      2  who will be my batchmates at the course of stu...\n",
      "1004      2              who will be my batchmates at the test\n",
      "1005      2  who will be my batchmates at the political pol...\n",
      "1006      2  who will be my batchmates at the political pol...\n",
      "1007      2          who will be my batchmates at the gun show,       label                                           sentence\n",
      "1008      2  who will be my batchmates at the calculator pr...\n",
      "1009      2         who will be my batchmates at the programme\n",
      "1010      2         who will be my batchmates at the programme\n",
      "1011      2  who will be my batchmates at the computer prog...\n",
      "1012      2           who will be my batchmates at the program\n",
      "1013      2           who will be my batchmates at the program\n",
      "1014      2           who will be my batchmates at the program\n",
      "1015      2         who will be my batchmates at the programme\n",
      "1016      2  who will be my batchmates at the course of fie...\n",
      "1017      2        who will be my batchmates at the curriculum\n",
      "1018      2        who will be my batchmates at the curriculum\n",
      "1019      2  who will be my batchmates at the political pla...\n",
      "1020      2  who will be my batchmates at the course of stu...\n",
      "1021      2  who will be my batchmates at the course of stu...\n",
      "1022      2   who will be my batchmates at the board of survey\n",
      "1023      2        who will be my batchmates at the curriculum\n",
      "1024      2         who will be my batchmates at the programme\n",
      "1025      2         who will be my batchmates at the programme\n",
      "1026      2  who will be my batchmates at the course of the...\n",
      "1027      2  who will be my batchmates at the course of my ...\n",
      "1028      2           who will be my batchmates at the program\n",
      "1029      2           who will be my batchmates at the program\n",
      "1030      2          who will be my batchmates at the gun show\n",
      "1031      2  who will be my batchmates at the reckoner prog...,       label                                           sentence\n",
      "1032      2  who will be my batchmates at the reckoner prog...\n",
      "1033      2  who will be my batchmates at the ramp up compu...\n",
      "1034      2  who will be my batchmates at the computer comp...\n",
      "1035      2  who will be my batchmates at the computer comp...\n",
      "1036      2  who will be my batchmates at the data processo...\n",
      "1037      2  who will be my batchmates at the calculator pr...\n",
      "1038      2  who will be my batchmates at the calculator pr...\n",
      "1039      2  who will be my batchmates at the course of the...\n",
      "1040      2  who will be my batchmates at the computer program\n",
      "1041      2  who will be my batchmates at the computer program\n",
      "1042      2           who will be my batchmates at the program\n",
      "1043      2  who will be my batchmates at the calculator pr...\n",
      "1044      2  who will be my batchmates at the calculator pr...\n",
      "1045      2  who will be my batchmates at the calculator pr...\n",
      "1046      2  who will be my batchmates at the reckoner prog...\n",
      "1047      2  who will be my batchmates at the reckoner prog...\n",
      "1048      2         who will be my batchmates at the programme\n",
      "1049      2  who will be my batchmates at the computer syll...\n",
      "1050      2  who will be my batchmates at the computer syll...\n",
      "1051      2  who will be my batchmates at the operating roo...\n",
      "1052      2  who will be my batchmates at the computer cour...\n",
      "1053      2  who will be my batchmates at the computer cour...\n",
      "1054      2  who will be my batchmates at the course of survey\n",
      "1055      2  who will be my batchmates at the data processo...,       label                                           sentence\n",
      "1056      2  who will be my batchmates at the data processo...\n",
      "1057      2  who will be my batchmates at the course of gra...\n",
      "1058      2  who will be my batchmates at the estimator pro...\n",
      "1059      2  who will be my batchmates at the estimator pro...\n",
      "1060      2  who will be my batchmates at the political pol...\n",
      "1061      2  who will be my batchmates at the reckoner prog...\n",
      "1062      2  who will be my batchmates at the reckoner prog...\n",
      "1063      2              who will be my batchmates at the test\n",
      "1064      2  who will be my batchmates at the electronic co...\n",
      "1065      2  who will be my batchmates at the electronic co...\n",
      "1066      2  who will be my batchmates at the political pol...\n",
      "1067      2  who will be my batchmates at the course of the...\n",
      "1068      2  who will be my batchmates at the course of stu...\n",
      "1069      2              who will be my batchmates at the exam\n",
      "1070      2  who will be my batchmates at the course of stu...\n",
      "1071      2  who will be my batchmates at the course of sub...\n",
      "1072      2  who will be my batchmates at the political cou...\n",
      "1073      2  who will be my batchmates at the course of survey\n",
      "1074      2  who will be my batchmates at the course of wor...\n",
      "1075      2  who will be my batchmates at the calculator pr...\n",
      "1076      2  who will be my batchmates at the course of stu...\n",
      "1077      2  who will be my batchmates at the course of ana...\n",
      "1078      2  who will be my batchmates at the course of stu...\n",
      "1079      2  who will be my batchmates at the course of the...,       label                                           sentence\n",
      "1080      2  who will be my batchmates at the course of survey\n",
      "1081      2  who will be my batchmates at the course of the...\n",
      "1082      2  who will be my batchmates at the course of fie...\n",
      "1083      2  who will be my batchmates at the course of con...\n",
      "1084      2         who will be my batchmates at the programme\n",
      "1085      2     who will be my batchmates at the path of study\n",
      "1086      2  who will be my batchmates at the path of study...\n",
      "1087      2  who will be my batchmates at the course of fie...\n",
      "1088      2    who will be my batchmates at the track of study\n",
      "1089      2    who will be my batchmates at the track of study\n",
      "1090      2         who will be my batchmates at the broadcast\n",
      "1091      2  who will be my batchmates at the course of stu...\n",
      "1092      2  who will be my batchmates at the course of survey\n",
      "1093      2  who will be my batchmates at the course of fie...\n",
      "1094      2  who will be my batchmates at the course of my ...\n",
      "1095      2  who will be my batchmates at the course of ins...\n",
      "1096      2         who will be my batchmates at the programme\n",
      "1097      2  who will be my batchmates at the course of exa...\n",
      "1098      2   who will be my batchmates at the course of learn\n",
      "1099      2  who will be my batchmates at the course of the...\n",
      "1100      2     who will be my batchmates at the form of study\n",
      "1101      2     who will be my batchmates at the form of study\n",
      "1102      2      is there anyone who can help me when i embark\n",
      "1103      2  is there anyone who can help me when i go to s...,       label                                           sentence\n",
      "1104      2     is there anyone who can help me when i draw in\n",
      "1105      2  is there anyone who can serve me when i go to ...\n",
      "1106      2  is there anyone who can help me when i build t...\n",
      "1107      2  is there anyone who can help me when i bring i...\n",
      "1108      2   is there anyone who can proffer any scholarships\n",
      "1109      2  is there anyone who can help me when i go to s...\n",
      "1110      2    is there anyone who can help me when i drive in\n",
      "1111      2      is there anyone who can help me when i get in\n",
      "1112      2  is there anyone who can supporter me when i ru...\n",
      "1113      2  is there anyone who can supporter me when i ge...\n",
      "1114      2  is there anyone who can help me when i get to ...\n",
      "1115      2      is there anyone who can help me when i get in\n",
      "1116      2  is there anyone who can help me when i begin t...\n",
      "1117      2  is there anyone who can assistance me when i r...\n",
      "1118      2     is there anyone who can help me when i grow in\n",
      "1119      2  is there anyone who can help me when i grow up...\n",
      "1120      2  is there anyone who can facilitate me when i g...\n",
      "1121      2  is there anyone who can serve me when i embark...\n",
      "1122      2  is there anyone who can serve me when i go sov...\n",
      "1123      2  is there anyone who can help me when i turn in...\n",
      "1124      2  is there anyone who can assistant me when i go...\n",
      "1125      2  is there anyone who can assistant me when i go...\n",
      "1126      2    is there anyone who can serve me when i come in\n",
      "1127      2     is there anyone who can help me when i get far,       label                                           sentence\n",
      "1128      2  is there anyone who can help me when i go golf...\n",
      "1129      2  is there anyone who can assist me when i go to...\n",
      "1130      2  is there anyone who can assistance me when i l...\n",
      "1131      2  is there anyone who can assistance me when i g...\n",
      "1132      2  is there anyone who can serve me when im go ov...\n",
      "1133      2  is there anyone who can helper me when i run s...\n",
      "1134      2  is there anyone who can helper me when i go to...\n",
      "1135      2      is there anyone who can help me when i get in\n",
      "1136      2  is there anyone who can assist me when i hit t...\n",
      "1137      2    is there anyone who can assist me when i go far\n",
      "1138      2  is there anyone who can facilitate me when im ...\n",
      "1139      2      is there anyone who can help me when i get in\n",
      "1140      2  is there anyone who can help me when i kick of...\n",
      "1141      2  is there anyone who can help me when i live in...\n",
      "1142      2  is there anyone who can help me when i live in...\n",
      "1143      2  is there anyone who can help me when i get int...\n",
      "1144      2  is there anyone who can help me when i run sho...\n",
      "1145      2  is there anyone who can help me when i go to r...\n",
      "1146      2  is there anyone who can help me when i turn 18...\n",
      "1147      2  is there anyone who can service me when i go t...\n",
      "1148      2  is there anyone who can help me when i get to ...\n",
      "1149      2  is there anyone who can help me when i snuff i...\n",
      "1150      2    is there anyone who can help me when i lead far\n",
      "1151      2  is there anyone who can help me when i hit the...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 93\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m#test \u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m#test_accuracy = test_evaluate(QModel_Classifier,cross_model_path, test_dataloader,hidden=hidden,num_labels=num_classes,feature_remove_max=True)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         bot\u001b[38;5;241m.\u001b[39mpolling(none_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre-process phase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-large\u001b[39m\u001b[38;5;124m'\u001b[39m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m---> 77\u001b[0m train_dataloader, val_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43maug_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43maug_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_aug\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msecond_num_aug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# without contrasive loss\u001b[39;00m\n\u001b[1;32m     82\u001b[0m bert_classifier, optimizer \u001b[38;5;241m=\u001b[39m initialize_model(QModel_Classifier,hidden\u001b[38;5;241m=\u001b[39mhidden,num_labels \u001b[38;5;241m=\u001b[39m num_classes)\n",
      "Cell \u001b[0;32mIn[7], line 173\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(tokenizer, train_ds, val_ds, aug_path, sample_num, seed, all, aug, aug_num)\u001b[0m\n\u001b[1;32m    169\u001b[0m     val_text[i] \u001b[38;5;241m=\u001b[39m text_preprocessing(val_text[i])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Run function `preprocessing_for_bert` on the train set and the validation set\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# print('Tokenizing data...')\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m train_inputs, train_masks \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_for_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m val_inputs, val_masks \u001b[38;5;241m=\u001b[39m preprocessing_for_bert(val_text, MAX_LEN)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Convert other data types to torch.Tensor\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 84\u001b[0m, in \u001b[0;36mpreprocessing_for_bert\u001b[0;34m(data, MAX_LEN)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# For every sentence...\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# `encode_plus` will:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m#    (1) Tokenize the sentence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#    (5) Create attention mask\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m#    (6) Return a dictionary of outputs\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     encoded_sent \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m     85\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext_preprocessing(sent),  \u001b[38;5;66;03m# Preprocess sentence\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Add `[CLS]` and `[SEP]`\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mMAX_LEN,  \u001b[38;5;66;03m# Max length to truncate/pad\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         pad_to_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Pad sentence to max length\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Return attention mask\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Add the outputs to the lists\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(encoded_sent\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from eda import gen_eda\n",
    "from cPosGpt2 import train_cposgpt2_and_augment\n",
    "def main():\n",
    "    MAX_LEN = 40\n",
    "    epoch =40 # number of epochs\n",
    "\n",
    "    scl = True  # if True -> scl + cross entropy loss. else just cross entropy loss\n",
    "    temprature = 0.3  # temprature for contrastive loss\n",
    "    lam = 0.9  # lambda for loss\n",
    "    patience=12 # early stop\n",
    "    hidden = 512\n",
    "    seed = 49    # seed\n",
    "\n",
    "    # for eda\n",
    "    alpha = 0.1\n",
    "    num_aug = 6\n",
    "    # for pos_gpt2\n",
    "    second_num_aug = 4\n",
    "    \n",
    "    #main bot augmentation file\n",
    "    output_file = 'main_bot'\n",
    "    os.makedirs(output_file, exist_ok=True)\n",
    "    \n",
    "   \n",
    "                \n",
    "    #eda\n",
    "    print('eda phase')\n",
    "    \n",
    "    file_name = 'eda.tsv'\n",
    "    output_dir = os.path.join(output_file, file_name)\n",
    "    gen_eda(df,output_dir , alpha=alpha, num_aug=num_aug , reverse = False)\n",
    "    \n",
    "    #from transformers import GPTNeoForCausalLM\n",
    "    from transformers import GPT2Tokenizer\n",
    "    from transformers import GPT2LMHeadModel\n",
    "    \n",
    "    #gpt2\n",
    "    print('GPT phase')\n",
    "    \n",
    "    \n",
    "    GPT2_MODEL = 'gpt2'\n",
    "    model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL,\n",
    "                                            cache_dir='transformers_cache')\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL,\n",
    "                                                  do_lower_case=True,\n",
    "                                              cache_dir='transformers_cache')\n",
    "    #GPT2_MODEL = 'EleutherAI/gpt-neo-1.3B' \n",
    "    #model = GPTNeoForCausalLM.from_pretrained(GPT2_MODEL,cache_dir='transformers_cache')\n",
    "\n",
    "    #eda + gpt2     \n",
    "\n",
    "    file_name = 'posgpt2_eda.tsv'    \n",
    "    x = f'{output_file}/eda.tsv'\n",
    "    train_df = pd.read_csv(x,sep='\\t')\n",
    "\n",
    "    sample = 3    \n",
    "    val_df = [train_df.loc[train_df.label == i].sample(n=sample, random_state=seed) for i in\n",
    "                    train_df.label.unique()]\n",
    "    val_df = pd.concat(val_df, axis=0).sample(frac=1)\n",
    "\n",
    "    \n",
    "    train_cposgpt2_and_augment(model,tokenizer,train_df,val_df,output=output_file,\n",
    "                               file_name=file_name,seed = 1234,max_seq_length = 64,sample_num=second_num_aug,num_train_epochs=10)\n",
    "    \n",
    "    \n",
    "    aug_path = f'{output_file}/posgpt2_eda.tsv'\n",
    "\n",
    "\n",
    "\n",
    "    set_seed(seed)\n",
    "    print('pre-process phase')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True) \n",
    "    train_dataloader, val_dataloader = prepare_data(tokenizer,df,None,aug_path, sample_num=10\\\n",
    "                                                                         , seed = seed , all=True,aug=True,aug_num = num_aug*second_num_aug)\n",
    "    \n",
    "    # without contrasive loss\n",
    "    \n",
    "    bert_classifier, optimizer = initialize_model(QModel_Classifier,hidden=hidden,num_labels = num_classes)\n",
    "    scl = False\n",
    "\n",
    "    print('training phase')\n",
    "    val_loss, val_accuracy = train(bert_classifier, train_dataloader, temprature, lam, scl, epoch ,val_dataloader, evaluation=True,patience=patience)\n",
    "\n",
    "    #test \n",
    "    #test_accuracy = test_evaluate(QModel_Classifier,cross_model_path, test_dataloader,hidden=hidden,num_labels=num_classes,feature_remove_max=True)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "        main()\n",
    "        bot.polling(none_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833b08b-1afa-4cdb-9cb5-a63d3cdd6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.6_1/libexec/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cPosGpt2 training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Dev loss 6.58949613571167\n",
      "Saving model. Best dev so far 6.58949613571167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▋                                 | 1/10 [00:47<07:08, 47.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Dev loss 5.886806488037109\n",
      "Saving model. Best dev so far 5.886806488037109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|███████▍                             | 2/10 [02:49<12:10, 91.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Dev loss 5.282986402511597\n",
      "Saving model. Best dev so far 5.282986402511597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███████████                          | 3/10 [04:28<11:04, 94.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Dev loss 4.9774158000946045\n",
      "Saving model. Best dev so far 4.9774158000946045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|██████████████▍                     | 4/10 [06:34<10:43, 107.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Dev loss 4.588931083679199\n",
      "Saving model. Best dev so far 4.588931083679199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|██████████████████▌                  | 5/10 [07:56<08:10, 98.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Dev loss 4.2994630336761475\n",
      "Saving model. Best dev so far 4.2994630336761475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████████████████████▏              | 6/10 [09:34<06:32, 98.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Dev loss 4.016672968864441\n",
      "Saving model. Best dev so far 4.016672968864441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|█████████████████████████▏          | 7/10 [11:59<05:40, 113.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Dev loss 3.7483785152435303\n",
      "Saving model. Best dev so far 3.7483785152435303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|█████████████████████████████▌       | 8/10 [13:05<03:16, 98.10s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "#gpt2\n",
    "print('GPT phase')\n",
    "\n",
    "\n",
    "GPT2_MODEL = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL,\n",
    "                                        cache_dir='transformers_cache')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL,\n",
    "                                              do_lower_case=True,\n",
    "                                          cache_dir='transformers_cache')\n",
    "#GPT2_MODEL = 'EleutherAI/gpt-neo-1.3B' \n",
    "#model = GPTNeoForCausalLM.from_pretrained(GPT2_MODEL,cache_dir='transformers_cache')\n",
    "\n",
    "#eda + gpt2     \n",
    "\n",
    "file_name = 'posgpt2_eda.tsv'    \n",
    "x = f'{output_file}/eda.tsv'\n",
    "train_df = pd.read_csv(x,sep='\\t')\n",
    "\n",
    "sample = 3    \n",
    "val_df = [train_df.loc[train_df.label == i].sample(n=sample, random_state=15) for i in\n",
    "                train_df.label.unique()]\n",
    "val_df = pd.concat(val_df, axis=0).sample(frac=1)\n",
    "\n",
    "\n",
    "train_cposgpt2_and_augment(model,tokenizer,train_df,val_df,output=output_file,\n",
    "                           file_name=file_name,seed = 1234,max_seq_length = 64,sample_num=second_num_aug,num_train_epochs=10)\n",
    "\n",
    "\n",
    "aug_path = f'{output_file}/posgpt2_eda.tsv'\n",
    "\n",
    "df = pd.read_csv(aug_path,sep ='\\t')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e22906a-1c8c-40ac-8d51-e80c3e24914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eda phase\n",
      "generated augmented sentences with eda for to main_bot/eda.tsv with num_aug=6\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "num_aug = 6\n",
    "# for pos_gpt2\n",
    "second_num_aug = 4\n",
    "\n",
    "#main bot augmentation file\n",
    "output_file = 'main_bot'\n",
    "os.makedirs(output_file, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "#eda\n",
    "print('eda phase')\n",
    "\n",
    "file_name = 'eda.tsv'\n",
    "output_dir = os.path.join(output_file, file_name)\n",
    "gen_eda(df,output_dir , alpha=alpha, num_aug=num_aug , reverse = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cc4be5e-b7f2-4667-afb9-6cc61d789351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_path = f'main_bot/eda.tsv'\n",
    "df = pd.read_csv(aug_path,sep ='\\t')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd556c0e-c8e9-4fb4-be22-14eea6927a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "class Args():\n",
    "    embedding_dim = 1024\n",
    "    hidden=512 \n",
    "    num_labels = 24\n",
    "    dropout=0.1\n",
    "    \n",
    "args = Args()\n",
    "PATH = r\"itmo_model.pt\"\n",
    "model = QModel_Classifier(args.embedding_dim,args.hidden,args.num_labels,args.dropout)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc145eb-179f-44a6-aa40-4f3c45ccbe64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
