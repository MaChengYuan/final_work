{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68126e9-cb97-43a5-8ba9-b0ee4f23714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import init\n",
    "import math\n",
    "from math import gcd\n",
    "from math import sqrt\n",
    "from torch.nn import functional as F\n",
    "import pymongo\n",
    "\n",
    "\n",
    "class PHMLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features, out_features,n=2):\n",
    "    super(PHMLayer, self).__init__()\n",
    "    self.n = n\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "\n",
    "    self.bias = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "    self.a = torch.zeros((n, n, n))\n",
    "    self.a = Parameter(torch.nn.init.xavier_uniform_(self.a))\n",
    "\n",
    "    self.s = torch.zeros((n, self.out_features//n, self.in_features//n)) \n",
    "    self.s = Parameter(torch.nn.init.xavier_uniform_(self.s))\n",
    "\n",
    "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
    "\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "  def kronecker_product1(self, a, b):\n",
    "\n",
    "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
    "    \n",
    "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
    "    siz0 = res.shape[:-4]\n",
    "    out = res.reshape(siz0 + siz1)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
    "\n",
    "    input = input.type(dtype=self.weight.type())\n",
    "\n",
    "      \n",
    "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'in_features={}, out_features={}, bias={}'.format(\n",
    "      self.in_features, self.out_features, self.bias is not None)\n",
    "    \n",
    "  def reset_parameters(self) -> None:\n",
    "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
    "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7378457b-a42d-439d-a9e8-ae53a305eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import init\n",
    "import math\n",
    "from math import gcd\n",
    "from math import sqrt\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class PHMLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, in_features, out_features,n=2):\n",
    "    super(PHMLayer, self).__init__()\n",
    "    self.n = n\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "\n",
    "    self.bias = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "    self.a = torch.zeros((n, n, n))\n",
    "    self.a = Parameter(torch.nn.init.xavier_uniform_(self.a))\n",
    "\n",
    "    self.s = torch.zeros((n, self.out_features//n, self.in_features//n)) \n",
    "    self.s = Parameter(torch.nn.init.xavier_uniform_(self.s))\n",
    "\n",
    "    self.weight = torch.zeros((self.out_features, self.in_features))\n",
    "\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "  def kronecker_product1(self, a, b):\n",
    "\n",
    "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
    "    \n",
    "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
    "    siz0 = res.shape[:-4]\n",
    "    out = res.reshape(siz0 + siz1)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    self.weight = torch.sum(self.kronecker_product1(self.a, self.s), dim=0)\n",
    "\n",
    "    input = input.type(dtype=self.weight.type())\n",
    "\n",
    "      \n",
    "    return F.linear(input, weight=self.weight, bias=self.bias)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'in_features={}, out_features={}, bias={}'.format(\n",
    "      self.in_features, self.out_features, self.bias is not None)\n",
    "    \n",
    "  def reset_parameters(self) -> None:\n",
    "    init.kaiming_uniform_(self.a, a=math.sqrt(5))\n",
    "    init.kaiming_uniform_(self.s, a=math.sqrt(5))\n",
    "    fan_in, _ = init._calculate_fan_in_and_fan_out(self.placeholder)\n",
    "    bound = 1 / math.sqrt(fan_in)\n",
    "    init.uniform_(self.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6a55ff-cf0a-4e2c-8142-fe33b676889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "class Model_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout):\n",
    "        super(Model_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "        self.Drop = nn.Dropout(self.dropout)\n",
    "        self.linear2 = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        \n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_dim),\n",
    "            # nn.Dropout(self.dropout),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0]\n",
    "\n",
    "        last_hidden_state_cls = self.linear(last_hidden_state_cls)\n",
    "\n",
    "        last_hidden_state_cls = self.Drop(last_hidden_state_cls)\n",
    "        \n",
    "\n",
    "        logits = self.linear2(last_hidden_state_cls)[:, 0, :]\n",
    "\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits, last_hidden_state_cls,outputs[0]\n",
    "class QModel_Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels, dropout,feature_remove_max= True):\n",
    "        super(QModel_Classifier, self).__init__()\n",
    "        # Instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-large')\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "        divisors = sorted(self.cf(embedding_dim,hidden_dim))\n",
    "        divisors1 = sorted(self.cf(hidden_dim,num_labels))\n",
    "        common_divisors = sorted(set(divisors1) & set(divisors))\n",
    "        if(feature_remove_max == True):\n",
    "            self.n = common_divisors[-1]\n",
    "        else :\n",
    "            self.n = common_divisors[0]\n",
    "        \n",
    "        self.linear = PHMLayer(self.embedding_dim, self.hidden_dim,self.n)\n",
    "        self.Drop = nn.Dropout(self.dropout)\n",
    "        self.linear2 = PHMLayer(self.hidden_dim, self.num_labels,self.n)\n",
    "        \n",
    "\n",
    "    def cf(self,num1,num2):\n",
    "            n=[]\n",
    "            g=gcd(num1, num2)\n",
    "            for i in range(1, int(sqrt(g))+1):\n",
    "                if g%i==0:\n",
    "                    n.append(i)\n",
    "                    if g!=i*i:\n",
    "                        n.append(int(g/i))\n",
    "            return n\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "\n",
    "        last_hidden_state_cls = outputs[0]\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "        last_hidden_state_cls = self.linear(last_hidden_state_cls)\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "        last_hidden_state_cls = self.Drop(last_hidden_state_cls)\n",
    "        #print(last_hidden_state_cls.shape)\n",
    "\n",
    "        logits = self.linear2(last_hidden_state_cls)[:, 0, :]\n",
    "        #print(logits.shape)\n",
    "        # Feed input to classifier to compute logits\n",
    "        #logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits, last_hidden_state_cls,outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2537bbee-dd05-436f-9439-13bec012ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import nn, optim, tensor\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    # torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "# set parameters\n",
    "\n",
    "\n",
    "scl_model_path = r\"itmo_model.pt\"\n",
    "cross_model_path = r\"itmo_model.pt\"\n",
    "\n",
    "\n",
    "# a function for preprocessing text\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(tokenizer,data, MAX_LEN):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,  # Max length to truncate/pad\n",
    "            padding='max_length',  # Pad sentence to max length\n",
    "            return_attention_mask=True  # Return attention mask\n",
    "        )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "# preparing data\n",
    "def prepare_data(tokenizer,train_ds,val_ds=None,aug_path = None,sample_num = 10 , seed = 32, all=True , aug = False,aug_num = 6):\n",
    "    # load data\n",
    "    # for tsv\n",
    "    # there is no valid_path\n",
    "    \n",
    "    global num_classes\n",
    "    num_classes = len(train_ds['label'].unique())\n",
    "    \n",
    "    #original one\n",
    "    if(all == False):\n",
    "        train_df = [train_ds.loc[train_ds.label == i].sample(n=sample, random_state=seed) for i in\n",
    "                    train_ds.label.unique()]\n",
    "        train_df = pd.concat(train_df, axis=0).sample(frac=1)\n",
    "    else :\n",
    "        #for all\n",
    "        train_df = train_ds\n",
    "    \n",
    "    train_df = train_df[['sentence','label']]\n",
    "    print(f'original is {len(train_df)}')\n",
    "    # data augmentation \n",
    "    if(aug == True):\n",
    "        indexs = train_df.index.values.tolist()\n",
    "        aug_df =  pd.read_csv(aug_path, sep='\\t')\n",
    "        #aug_df = [aug_ds[i*aug_num:i*aug_num+aug_num] for i inindexs]\n",
    "        \n",
    "        #print(aug_df[:aug_num*2])\n",
    "        #aug_df = pd.concat(aug_df, axis=0).sample(frac=1)\n",
    "        print(f'aug is {len(aug_df)}')\n",
    "        train_df = pd.concat([train_df,aug_df], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "        print(f'fianl is {len(train_df)}')\n",
    "    if(val_ds == None):\n",
    "        sample = 5\n",
    "        \n",
    "        val_df = [train_df.loc[train_df.label == i].sample(n=sample,replace = True, random_state=seed) for i in\n",
    "                    train_df.label.unique()]\n",
    "        val_df = pd.concat(val_df, axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        print(f'val_df is {len(val_df)}')\n",
    "\n",
    "        \n",
    "    # random 20 per class sample for validation\n",
    "\n",
    "\n",
    "    train_text = train_df[\"sentence\"].tolist()\n",
    "    train_label = train_df[\"label\"].tolist()\n",
    "    val_text = val_df[\"sentence\"].tolist()\n",
    "    val_label = val_df[\"label\"].tolist()\n",
    "    \n",
    "\n",
    "    # Concatenate train data and test data\n",
    "    all_text = np.concatenate([train_text, val_text], axis=0)\n",
    "\n",
    "    # Encode our concatenated data\n",
    "    encoded_text = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_text]\n",
    "    global MAX_LEN\n",
    "    # Find the maximum length\n",
    "    MAX_LEN = max([len(sent) for sent in encoded_text])\n",
    "\n",
    "    # preprocessing train data\n",
    "    for i in range(len(train_text)):\n",
    "        train_text[i] = text_preprocessing(train_text[i])\n",
    "\n",
    "    # preprocessing validation data\n",
    "    for i in range(len(val_text)):\n",
    "        val_text[i] = text_preprocessing(val_text[i])\n",
    "\n",
    "    # Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "    # print('Tokenizing data...')\n",
    "    train_inputs, train_masks = preprocessing_for_bert(tokenizer,train_text, MAX_LEN)\n",
    "    val_inputs, val_masks = preprocessing_for_bert(tokenizer,val_text, MAX_LEN)\n",
    "\n",
    "\n",
    "    # Convert other data types to torch.Tensor\n",
    "    train_labels = torch.tensor(train_label)\n",
    "    val_labels = torch.tensor(val_label)\n",
    "\n",
    "\n",
    "    # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "    batch_size = 16\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "\n",
    "def initialize_model(model,hidden = 16 , num_labels = 2 ,feature_remove_max=True):\n",
    "    \"\"\"Initialize the Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate Bert Classifier\n",
    "    if(model == QModel_Classifier):\n",
    "        model_classifier = model(1024, hidden_dim=hidden, num_labels = num_labels, dropout=0.1,feature_remove_max=feature_remove_max)\n",
    "    else:\n",
    "        model_classifier = model(1024, hidden_dim=hidden, num_labels = num_labels, dropout=0.1)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    model_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(model_classifier.parameters(),\n",
    "                      lr=4e-5,  # Default learning rate\n",
    "                      eps=1e-8  # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    return model_classifier, optimizer\n",
    "\n",
    "\n",
    "def contrastive_loss(temp, embedding, label):\n",
    "    \"\"\"calculate the contrastive loss\n",
    "    \"\"\"\n",
    "    nsamples, nx, ny = embedding.shape\n",
    "    embedding = embedding.reshape((nsamples,nx*ny))\n",
    "    \n",
    "    # cosine similarity between embeddings\n",
    "    cosine_sim = cosine_similarity(embedding, embedding)\n",
    "    # remove diagonal elements from matrix\n",
    "    dis = cosine_sim[~np.eye(cosine_sim.shape[0], dtype=bool)].reshape(cosine_sim.shape[0], -1)\n",
    "    # apply temprature to elements\n",
    "    dis = dis / temp\n",
    "    cosine_sim = cosine_sim / temp\n",
    "    # apply exp to elements\n",
    "    dis = np.exp(dis)\n",
    "    cosine_sim = np.exp(cosine_sim)\n",
    "\n",
    "    # calculate row sum\n",
    "    row_sum = []\n",
    "    for i in range(len(embedding)):\n",
    "        row_sum.append(sum(dis[i]))\n",
    "    # calculate outer sum\n",
    "    contrastive_loss = 0\n",
    "    for i in range(len(embedding)):\n",
    "        n_i = label.tolist().count(label[i]) - 1\n",
    "        inner_sum = 0\n",
    "        # calculate inner sum\n",
    "        for j in range(len(embedding)):\n",
    "            if label[i] == label[j] and i != j:\n",
    "                inner_sum = inner_sum + np.log(cosine_sim[i][j] / row_sum[i])\n",
    "        if n_i != 0:\n",
    "            contrastive_loss += (inner_sum / (-n_i))\n",
    "        else:\n",
    "            contrastive_loss += 0\n",
    "    return contrastive_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader, tem, lam, scl):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, h_s,_ = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        if scl:\n",
    "            cross_loss = loss_fn(logits, b_labels)\n",
    "            contrastive_l = contrastive_loss(tem, h_s.cpu().detach().numpy(), b_labels)\n",
    "            loss = (lam * contrastive_l) + (1 - lam) * (cross_loss)\n",
    "            val_loss.append(loss.item())\n",
    "        else:\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            #self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def train(model,optimizer,train_dataloader, tem, lam, scl,epoch = 40,val_dataloader=None, evaluation=False,patience = 25):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Specify loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    val_list = []\n",
    "    train_list = []\n",
    "    best_validation_loss = float('inf')\n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta=0)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        e = e + 1\n",
    "        print(\n",
    "            f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Train Accuracy':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\" * 86)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        train_accuracy = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits, hiden_state,_ = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the accuracy rate\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            train_accuracy.append(accuracy)\n",
    "\n",
    "            # Compute loss\n",
    "            if scl == True:\n",
    "                cross_loss = loss_fn(logits, b_labels)\n",
    "                contrastive_l = contrastive_loss(tem, hiden_state.cpu().detach().numpy(), b_labels)\n",
    "                loss = (lam * contrastive_l) + (1 - lam) * (cross_loss)\n",
    "            if scl == False:\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            print(\n",
    "                f\"{e:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {accuracy:^14.6} | {'-':^10} | {'-':^9} | {'-':^9.2}\")\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "\n",
    "        # Reset batch tracking variables\n",
    "        batch_loss, batch_counts = 0, 0\n",
    "        t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        total_accuracy = np.mean(train_accuracy)\n",
    "        train_list.append(avg_train_loss)\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader, tem, lam, scl)\n",
    "            val_list.append(val_loss)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(\"-\" * 86)\n",
    "            print(\n",
    "                f\"{'end':^7} | {'-':^7} | {avg_train_loss:^12.6f} | {total_accuracy:^14.6} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 86)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "        \n",
    "        if (val_loss < best_validation_loss) and scl == True:\n",
    "            best_validation_loss = val_loss\n",
    "            torch.save(model.state_dict(), scl_model_path)\n",
    "        elif (val_loss < best_validation_loss) and scl == False:\n",
    "            best_validation_loss = val_loss\n",
    "            torch.save(model.state_dict(), cross_model_path)\n",
    "\n",
    "\n",
    "        #early stopping\n",
    "        #print(early_stopper.counter)\n",
    "        if early_stopper.early_stop(val_loss):  \n",
    "            break\n",
    "\n",
    "\n",
    "    # plot train and valid loss\n",
    "    plt.plot(list(range(len(val_list))), val_list, label=\"validation loss\")\n",
    "    plt.plot(list(range(len(train_list))), train_list, label=\"training loss\")\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('number of epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    return best_validation_loss, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def test_evaluate(model,model_path, test_dataloader,hidden=16,num_labels=2,feature_remove_max=False):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our vtest set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    if(model == QModel_Classifier):\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1,feature_remove_max=feature_remove_max)\n",
    "    else:\n",
    "        model = model(1024,hidden, num_labels=num_classes, dropout=0.1)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_accuracy = []\n",
    "    predict = []\n",
    "    y_true = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, _ ,_= model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predict += preds.tolist()\n",
    "        y_true += b_labels.tolist()\n",
    "\n",
    "    # plot heatmap\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "    cm = confusion_matrix(y_true, predict)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sn.heatmap(cm, annot=True)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    print(f'Accuracy: {accuracy_score(y_true, predict)}')\n",
    "\n",
    "    # Recall\n",
    "    print(f'Recall: {recall_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # Precision\n",
    "    print(f'Precision: {precision_score(y_true, predict, average=None)}')\n",
    "\n",
    "    # F1_score\n",
    "    print(f'F1_score: {f1_score(y_true, predict, average=None)}')\n",
    "\n",
    "    return accuracy_score(y_true, predict)\n",
    "    \n",
    "#test_evaluate(cross_model_path, test_dataloader)\n",
    "#test_evaluate(scl_model_path, test_dataloader)\n",
    "\n",
    "# scl_test_acc = test_evaluate(scl_model_path, test_dataloader)\n",
    "\n",
    "def model_predict(model,hidden, model_path, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model = model(1024,hidden, num_classes, dropout=0.1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits, _,_ = model(b_input_ids, b_attn_mask)\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        all_logits += preds.tolist()\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    # all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    # probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    # predict = np.argmax(probs)\n",
    "\n",
    "    return all_logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c5a520-0ed7-47a8-94f9-20d9c09ea21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mongodb_atlas(table_name):\n",
    "\n",
    "    mydb = myclient[\"itmo_data\"]\n",
    "\n",
    "    mycol = mydb[table_name]\n",
    "\n",
    "    return mycol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103f529c-36a3-4e0e-95d1-e4a340228705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " final df 82\n",
      "eda phase\n",
      "generated augmented sentences with eda for to main_bot/eda.tsv with num_aug=6\n",
      "GPT phase\n",
      "pre-process phase\n",
      "original is 82\n",
      "aug is 1968\n",
      "fianl is 2050\n",
      "val_df is 115\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/mac/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training phase\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Accuracy |  Val Loss  |  Val Acc  |  Elapsed \n",
      "--------------------------------------------------------------------------------------\n",
      "   1    |    0    |   3.139546   |      0.0       |     -      |     -     |     -    \n",
      "   1    |    1    |   3.137340   |      0.0       |     -      |     -     |     -    \n",
      "   1    |    2    |   3.136039   |      12.5      |     -      |     -     |     -    \n",
      "   1    |    3    |   3.133080   |      12.5      |     -      |     -     |     -    \n",
      "   1    |    4    |   3.133739   |      0.0       |     -      |     -     |     -    \n",
      "   1    |    5    |   3.132885   |     18.75      |     -      |     -     |     -    \n",
      "   1    |    6    |   3.132707   |      0.0       |     -      |     -     |     -    \n",
      "   1    |    7    |   3.133138   |      0.0       |     -      |     -     |     -    \n",
      "   1    |    8    |   3.131982   |      12.5      |     -      |     -     |     -    \n",
      "   1    |    9    |   3.131060   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   10    |   3.131731   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   11    |   3.130987   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   12    |   3.131775   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   13    |   3.131171   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   14    |   3.131777   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   15    |   3.131615   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   16    |   3.131557   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   17    |   3.130977   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   18    |   3.130600   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   19    |   3.129030   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   20    |   3.127945   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   21    |   3.128029   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   22    |   3.128738   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   23    |   3.128676   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   24    |   3.128921   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   25    |   3.128937   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   26    |   3.128808   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   27    |   3.128490   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   28    |   3.128243   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   29    |   3.127433   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   30    |   3.126950   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   31    |   3.126546   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   32    |   3.125443   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   33    |   3.126604   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   34    |   3.127322   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   35    |   3.127435   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   36    |   3.127954   |      0.0       |     -      |     -     |     -    \n",
      "   1    |   37    |   3.127909   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   38    |   3.128184   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   39    |   3.127501   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   40    |   3.126989   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   41    |   3.126822   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   42    |   3.124813   |      25.0      |     -      |     -     |     -    \n",
      "   1    |   43    |   3.123845   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   44    |   3.122150   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   45    |   3.121717   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   46    |   3.120846   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   47    |   3.121330   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   48    |   3.119734   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   49    |   3.118848   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   50    |   3.118534   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   51    |   3.117608   |      6.25      |     -      |     -     |     -    \n",
      "   1    |   52    |   3.117000   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   53    |   3.116329   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   54    |   3.113930   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   55    |   3.110253   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   56    |   3.108924   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   57    |   3.107457   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   58    |   3.105772   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   59    |   3.104254   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   60    |   3.102644   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   61    |   3.101763   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   62    |   3.099289   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   63    |   3.096726   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   64    |   3.095224   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   65    |   3.093839   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   66    |   3.091352   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   67    |   3.087660   |     68.75      |     -      |     -     |     -    \n",
      "   1    |   68    |   3.085647   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   69    |   3.083598   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   70    |   3.081320   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   71    |   3.078843   |      25.0      |     -      |     -     |     -    \n",
      "   1    |   72    |   3.077403   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   73    |   3.074073   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   74    |   3.071142   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   75    |   3.066507   |      25.0      |     -      |     -     |     -    \n",
      "   1    |   76    |   3.063910   |      12.5      |     -      |     -     |     -    \n",
      "   1    |   77    |   3.059859   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   78    |   3.056863   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   79    |   3.052984   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   80    |   3.049234   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   81    |   3.044988   |     18.75      |     -      |     -     |     -    \n",
      "   1    |   82    |   3.040666   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   83    |   3.035729   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   84    |   3.029899   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   85    |   3.025310   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   86    |   3.022960   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   87    |   3.016415   |      25.0      |     -      |     -     |     -    \n",
      "   1    |   88    |   3.012050   |      75.0      |     -      |     -     |     -    \n",
      "   1    |   89    |   3.007653   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   90    |   3.002373   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   91    |   2.999731   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   92    |   2.994143   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   93    |   2.990619   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   94    |   2.984573   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   95    |   2.979669   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   96    |   2.976329   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   97    |   2.973290   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   98    |   2.968198   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   99    |   2.964157   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   100   |   2.959116   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   101   |   2.953325   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   102   |   2.948643   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   103   |   2.944087   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   104   |   2.939750   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   105   |   2.934207   |      37.5      |     -      |     -     |     -    \n",
      "   1    |   106   |   2.930629   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   107   |   2.924997   |      75.0      |     -      |     -     |     -    \n",
      "   1    |   108   |   2.919880   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   109   |   2.916393   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   110   |   2.912126   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   111   |   2.906112   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   112   |   2.902868   |     68.75      |     -      |     -     |     -    \n",
      "   1    |   113   |   2.898490   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   114   |   2.893376   |     68.75      |     -      |     -     |     -    \n",
      "   1    |   115   |   2.887192   |     68.75      |     -      |     -     |     -    \n",
      "   1    |   116   |   2.883758   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   117   |   2.877773   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   118   |   2.873159   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   119   |   2.868257   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   120   |   2.863059   |      62.5      |     -      |     -     |     -    \n",
      "   1    |   121   |   2.859568   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   122   |   2.856901   |     31.25      |     -      |     -     |     -    \n",
      "   1    |   123   |   2.851228   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   124   |   2.844021   |     56.25      |     -      |     -     |     -    \n",
      "   1    |   125   |   2.837870   |     68.75      |     -      |     -     |     -    \n",
      "   1    |   126   |   2.834709   |     43.75      |     -      |     -     |     -    \n",
      "   1    |   127   |   2.829471   |      50.0      |     -      |     -     |     -    \n",
      "   1    |   128   |   2.818717   |     100.0      |     -      |     -     |     -    \n",
      "--------------------------------------------------------------------------------------\n",
      "  end   |    -    |   2.818717   |    30.7655     |  2.241773  |   56.77   |  4869.59 \n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Accuracy |  Val Loss  |  Val Acc  |  Elapsed \n",
      "--------------------------------------------------------------------------------------\n",
      "   2    |    0    |   1.913929   |     81.25      |     -      |     -     |     -    \n",
      "   2    |    1    |   2.127701   |      50.0      |     -      |     -     |     -    \n",
      "   2    |    2    |   2.186246   |     43.75      |     -      |     -     |     -    \n",
      "   2    |    3    |   2.212617   |      50.0      |     -      |     -     |     -    \n",
      "   2    |    4    |   2.189773   |     56.25      |     -      |     -     |     -    \n",
      "   2    |    5    |   2.171594   |      62.5      |     -      |     -     |     -    \n",
      "   2    |    6    |   2.139901   |     81.25      |     -      |     -     |     -    \n",
      "   2    |    7    |   2.165313   |     31.25      |     -      |     -     |     -    \n",
      "   2    |    8    |   2.151705   |      37.5      |     -      |     -     |     -    \n",
      "   2    |    9    |   2.129959   |      50.0      |     -      |     -     |     -    \n",
      "   2    |   10    |   2.126823   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   11    |   2.134078   |     43.75      |     -      |     -     |     -    \n",
      "   2    |   12    |   2.125975   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   13    |   2.131146   |      50.0      |     -      |     -     |     -    \n",
      "   2    |   14    |   2.118685   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   15    |   2.110800   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   16    |   2.100843   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   17    |   2.077736   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   18    |   2.077603   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   19    |   2.069657   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   20    |   2.059067   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   21    |   2.057567   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   22    |   2.062953   |     43.75      |     -      |     -     |     -    \n",
      "   2    |   23    |   2.054309   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   24    |   2.046717   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   25    |   2.040348   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   26    |   2.034919   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   27    |   2.030248   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   28    |   2.013550   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   29    |   2.003906   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   30    |   2.004037   |      50.0      |     -      |     -     |     -    \n",
      "   2    |   31    |   1.999287   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   32    |   1.991595   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   33    |   1.988076   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   34    |   1.985739   |      50.0      |     -      |     -     |     -    \n",
      "   2    |   35    |   1.982811   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   36    |   1.975818   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   37    |   1.971671   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   38    |   1.971517   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   39    |   1.962817   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   40    |   1.956372   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   41    |   1.958832   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   42    |   1.947553   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   43    |   1.944620   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   44    |   1.934981   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   45    |   1.934622   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   46    |   1.926984   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   47    |   1.928118   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   48    |   1.924143   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   49    |   1.920236   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   50    |   1.917492   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   51    |   1.910547   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   52    |   1.907990   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   53    |   1.902610   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   54    |   1.900280   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   55    |   1.893429   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   56    |   1.891337   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   57    |   1.884370   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   58    |   1.883149   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   59    |   1.880922   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   60    |   1.872398   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   61    |   1.869544   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   62    |   1.860718   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   63    |   1.853012   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   64    |   1.847117   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   65    |   1.842384   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   66    |   1.836904   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   67    |   1.831334   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   68    |   1.826274   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   69    |   1.821236   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   70    |   1.816888   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   71    |   1.813090   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   72    |   1.811102   |     56.25      |     -      |     -     |     -    \n",
      "   2    |   73    |   1.804059   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   74    |   1.800442   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   75    |   1.796394   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   76    |   1.791652   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   77    |   1.787286   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   78    |   1.782073   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   79    |   1.779014   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   80    |   1.773235   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   81    |   1.766763   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   82    |   1.762411   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   83    |   1.758595   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   84    |   1.752593   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   85    |   1.751347   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   86    |   1.746783   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   87    |   1.742229   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   88    |   1.737858   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   89    |   1.733472   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   90    |   1.728536   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   91    |   1.724988   |      62.5      |     -      |     -     |     -    \n",
      "   2    |   92    |   1.718067   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   93    |   1.715944   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   94    |   1.711982   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   95    |   1.708348   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   96    |   1.703762   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   97    |   1.697576   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   98    |   1.691432   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   99    |   1.686672   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   100   |   1.679604   |     100.0      |     -      |     -     |     -    \n",
      "   2    |   101   |   1.675195   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   102   |   1.670213   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   103   |   1.666823   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   104   |   1.665162   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   105   |   1.662754   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   106   |   1.656789   |     100.0      |     -      |     -     |     -    \n",
      "   2    |   107   |   1.652610   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   108   |   1.649119   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   109   |   1.645371   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   110   |   1.639424   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   111   |   1.634289   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   112   |   1.628814   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   113   |   1.625068   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   114   |   1.621337   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   115   |   1.617612   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   116   |   1.616675   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   117   |   1.611137   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   118   |   1.606432   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   119   |   1.602584   |     81.25      |     -      |     -     |     -    \n",
      "   2    |   120   |   1.597640   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   121   |   1.594102   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   122   |   1.589458   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   123   |   1.587353   |      75.0      |     -      |     -     |     -    \n",
      "   2    |   124   |   1.585113   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   125   |   1.585360   |     68.75      |     -      |     -     |     -    \n",
      "   2    |   126   |   1.579086   |     93.75      |     -      |     -     |     -    \n",
      "   2    |   127   |   1.575133   |      87.5      |     -      |     -     |     -    \n",
      "   2    |   128   |   1.569859   |     100.0      |     -      |     -     |     -    \n",
      "--------------------------------------------------------------------------------------\n",
      "  end   |    -    |   1.569859   |    72.6744     |  1.132691  |   82.81   |  2612.84 \n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Accuracy |  Val Loss  |  Val Acc  |  Elapsed \n",
      "--------------------------------------------------------------------------------------\n",
      "   3    |    0    |   1.230788   |      87.5      |     -      |     -     |     -    \n",
      "   3    |    1    |   1.117075   |     93.75      |     -      |     -     |     -    \n",
      "   3    |    2    |   1.061122   |      87.5      |     -      |     -     |     -    \n",
      "   3    |    3    |   1.072328   |      75.0      |     -      |     -     |     -    \n",
      "   3    |    4    |   1.043048   |     93.75      |     -      |     -     |     -    \n",
      "   3    |    5    |   1.011678   |     93.75      |     -      |     -     |     -    \n",
      "   3    |    6    |   1.042061   |     68.75      |     -      |     -     |     -    \n",
      "   3    |    7    |   1.031719   |      87.5      |     -      |     -     |     -    \n",
      "   3    |    8    |   1.025635   |     81.25      |     -      |     -     |     -    \n",
      "   3    |    9    |   1.012626   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   10    |   1.017940   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   11    |   1.018196   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   12    |   1.031658   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   13    |   1.055894   |     68.75      |     -      |     -     |     -    \n",
      "   3    |   14    |   1.063961   |     68.75      |     -      |     -     |     -    \n",
      "   3    |   15    |   1.046587   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   16    |   1.033744   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   17    |   1.022905   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   18    |   1.023012   |      75.0      |     -      |     -     |     -    \n",
      "   3    |   19    |   1.020380   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   20    |   1.017459   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   21    |   1.003230   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   22    |   0.999887   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   23    |   0.988812   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   24    |   0.985245   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   25    |   0.976063   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   26    |   0.982128   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   27    |   0.982208   |      75.0      |     -      |     -     |     -    \n",
      "   3    |   28    |   0.972495   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   29    |   0.969786   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   30    |   0.970128   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   31    |   0.961753   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   32    |   0.960779   |      75.0      |     -      |     -     |     -    \n",
      "   3    |   33    |   0.953539   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   34    |   0.948203   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   35    |   0.940215   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   36    |   0.939923   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   37    |   0.932261   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   38    |   0.932789   |      75.0      |     -      |     -     |     -    \n",
      "   3    |   39    |   0.933203   |     68.75      |     -      |     -     |     -    \n",
      "   3    |   40    |   0.925533   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   41    |   0.927701   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   42    |   0.929959   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   43    |   0.930690   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   44    |   0.931377   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   45    |   0.928603   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   46    |   0.926876   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   47    |   0.924010   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   48    |   0.917252   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   49    |   0.911211   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   50    |   0.906438   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   51    |   0.900137   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   52    |   0.896183   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   53    |   0.892805   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   54    |   0.892292   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   55    |   0.889740   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   56    |   0.884992   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   57    |   0.881839   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   58    |   0.879071   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   59    |   0.876981   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   60    |   0.872580   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   61    |   0.865986   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   62    |   0.861740   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   63    |   0.856932   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   64    |   0.854700   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   65    |   0.851761   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   66    |   0.849243   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   67    |   0.843219   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   68    |   0.838203   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   69    |   0.834813   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   70    |   0.831347   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   71    |   0.828813   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   72    |   0.826391   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   73    |   0.827942   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   74    |   0.822361   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   75    |   0.818382   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   76    |   0.815870   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   77    |   0.811634   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   78    |   0.809559   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   79    |   0.804204   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   80    |   0.802091   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   81    |   0.796641   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   82    |   0.793696   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   83    |   0.789423   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   84    |   0.787103   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   85    |   0.784773   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   86    |   0.782969   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   87    |   0.779816   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   88    |   0.777032   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   89    |   0.775964   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   90    |   0.771486   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   91    |   0.768314   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   92    |   0.768155   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   93    |   0.765172   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   94    |   0.761268   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   95    |   0.759163   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   96    |   0.758087   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   97    |   0.755146   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   98    |   0.750318   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   99    |   0.748007   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   100   |   0.744444   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   101   |   0.740702   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   102   |   0.737743   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   103   |   0.734984   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   104   |   0.731056   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   105   |   0.726922   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   106   |   0.725301   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   107   |   0.724001   |      87.5      |     -      |     -     |     -    \n",
      "   3    |   108   |   0.722090   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   109   |   0.718515   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   110   |   0.715387   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   111   |   0.712587   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   112   |   0.710504   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   113   |   0.708049   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   114   |   0.704369   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   115   |   0.701730   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   116   |   0.700226   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   117   |   0.699139   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   118   |   0.696828   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   119   |   0.693563   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   120   |   0.692604   |     81.25      |     -      |     -     |     -    \n",
      "   3    |   121   |   0.690618   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   122   |   0.690029   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   123   |   0.687783   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   124   |   0.687016   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   125   |   0.684082   |     93.75      |     -      |     -     |     -    \n",
      "   3    |   126   |   0.680922   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   127   |   0.679088   |     100.0      |     -      |     -     |     -    \n",
      "   3    |   128   |   0.674419   |     100.0      |     -      |     -     |     -    \n",
      "--------------------------------------------------------------------------------------\n",
      "  end   |    -    |   0.674419   |    91.5213     |  0.357967  |   99.22   |  3070.95 \n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  | Train Accuracy |  Val Loss  |  Val Acc  |  Elapsed \n",
      "--------------------------------------------------------------------------------------\n",
      "   4    |    0    |   0.477237   |      87.5      |     -      |     -     |     -    \n",
      "   4    |    1    |   0.474842   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    2    |   0.460676   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    3    |   0.437916   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    4    |   0.463974   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    5    |   0.431320   |     100.0      |     -      |     -     |     -    \n",
      "   4    |    6    |   0.410893   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    7    |   0.401985   |     93.75      |     -      |     -     |     -    \n",
      "   4    |    8    |   0.377802   |     100.0      |     -      |     -     |     -    \n",
      "   4    |    9    |   0.365992   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   10    |   0.369452   |      87.5      |     -      |     -     |     -    \n",
      "   4    |   11    |   0.377612   |     93.75      |     -      |     -     |     -    \n",
      "   4    |   12    |   0.382637   |     93.75      |     -      |     -     |     -    \n",
      "   4    |   13    |   0.372948   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   14    |   0.375734   |      87.5      |     -      |     -     |     -    \n",
      "   4    |   15    |   0.371144   |     93.75      |     -      |     -     |     -    \n",
      "   4    |   16    |   0.375129   |      87.5      |     -      |     -     |     -    \n",
      "   4    |   17    |   0.362430   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   18    |   0.355783   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   19    |   0.353852   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   20    |   0.347642   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   21    |   0.347315   |     93.75      |     -      |     -     |     -    \n",
      "   4    |   22    |   0.344389   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   23    |   0.341294   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   24    |   0.338943   |     93.75      |     -      |     -     |     -    \n",
      "   4    |   25    |   0.332365   |     100.0      |     -      |     -     |     -    \n",
      "   4    |   26    |   0.329983   |     93.75      |     -      |     -     |     -    \n"
     ]
    }
   ],
   "source": [
    "from eda import gen_eda\n",
    "from cPosGpt2 import train_cposgpt2_and_augment\n",
    "\n",
    "epoch =20 # number of epochs\n",
    "\n",
    "scl = True  # if True -> scl + cross entropy loss. else just cross entropy loss\n",
    "temprature = 0.3  # temprature for contrastive loss\n",
    "lam = 0.9  # lambda for loss\n",
    "patience=12 # early stop\n",
    "hidden = 512\n",
    "seed = 49    # seed\n",
    "\n",
    "# for eda\n",
    "alpha = 0.1\n",
    "num_aug = 6\n",
    "# for pos_gpt2\n",
    "second_num_aug = 4\n",
    "\n",
    "\n",
    "# original datasets plus new_responses \n",
    "\n",
    "\n",
    "token2 = 'mongodb+srv://mongo:mongo@cluster0.gcj8po2.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "\n",
    "        \n",
    "myclient = pymongo.MongoClient(token2)\n",
    "\n",
    "\n",
    "mycol = mongodb_atlas('training_data')\n",
    "x = mycol.find()    \n",
    "df = pd.DataFrame(list(x))\n",
    "df = df.rename(columns={'tag':'label','patterns':'sentence'})\n",
    "df = df[['label','sentence']]\n",
    "\n",
    "#dropna in case \n",
    "df = df.dropna(subset = ['sentence']).reset_index(drop=True)\n",
    "print(f' final df {len(df)}')\n",
    "\n",
    "\n",
    "#main bot augmentation file\n",
    "output_file = 'main_bot'\n",
    "os.makedirs(output_file, exist_ok=True)\n",
    "\n",
    "\n",
    "            \n",
    "#eda\n",
    "print('eda phase')\n",
    "\n",
    "file_name = 'eda.tsv'\n",
    "output_dir = os.path.join(output_file, file_name)\n",
    "gen_eda(df,output_dir , alpha=alpha, num_aug=num_aug , reverse = False)\n",
    "\n",
    "from transformers import GPTNeoForCausalLM\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "#gpt2\n",
    "print('GPT phase')\n",
    "\n",
    "\n",
    "GPT2_MODEL = 'gpt2'\n",
    "#model = GPT2LMHeadModel.from_pretrained(GPT2_MODEL,cache_dir='transformers_cache')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL,\n",
    "                                              do_lower_case=True,\n",
    "                                          cache_dir='transformers_cache')\n",
    "\n",
    "GPT2_MODEL = 'EleutherAI/gpt-neo-1.3B' \n",
    "#model = GPTNeoForCausalLM.from_pretrained(GPT2_MODEL,cache_dir='transformers_cache')\n",
    "\n",
    "#eda + gpt2     \n",
    "\n",
    "file_name = 'posgpt2_eda.tsv'    \n",
    "x = f'{output_file}/eda.tsv'\n",
    "train_df = pd.read_csv(x,sep='\\t')\n",
    "\n",
    "sample = 3    \n",
    "val_df = [train_df.loc[train_df.label == i].sample(n=sample, random_state=seed) for i in\n",
    "                train_df.label.unique()]\n",
    "val_df = pd.concat(val_df, axis=0).sample(frac=1)\n",
    "\n",
    "\n",
    "#train_cposgpt2_and_augment(model,tokenizer,train_df,val_df,output=output_file,file_name=file_name,seed = 1234,max_seq_length = MAX_LEN,sample_num=second_num_aug,num_train_epochs=3)\n",
    "\n",
    "\n",
    "aug_path = f'{output_file}/posgpt2_eda.tsv'\n",
    "\n",
    "\n",
    "\n",
    "set_seed(seed)\n",
    "print('pre-process phase')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True) \n",
    "train_dataloader, val_dataloader = prepare_data(tokenizer,df,None,aug_path, sample_num=10\\\n",
    "                                                                     , seed = seed , all=True,aug=True,aug_num = num_aug*second_num_aug)\n",
    "\n",
    "print(num_classes)\n",
    "# without contrasive loss\n",
    "bert_classifier, optimizer = initialize_model(QModel_Classifier,hidden=hidden,num_labels = num_classes)\n",
    "scl = False\n",
    "\n",
    "print('training phase')\n",
    "val_loss, val_accuracy = train(bert_classifier,optimizer, train_dataloader, temprature, lam, scl, epoch ,val_dataloader, evaluation=True,patience=patience)\n",
    "\n",
    "#test \n",
    "#test_accuracy = test_evaluate(QModel_Classifier,cross_model_path, test_dataloader,hidden=hidden,num_labels=num_classes,feature_remove_max=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e0724-4a8b-434e-aab3-3e34833e51c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006366d3-9f67-4275-8a7c-a2f4b842b9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da8997-6390-427b-82a9-53e157a0e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = 'mongodb+srv://mongo:mongo@cluster0.gcj8po2.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'       \n",
    "myclient = pymongo.MongoClient(token2)\n",
    "\n",
    "mycol = mongodb_atlas('new_response')\n",
    "x = mycol.find()    \n",
    "df = pd.DataFrame(list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c464ae-5255-4317-9458-af89dcd910b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id.dropna(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254f25f-e8b6-4aa3-8fcd-1df263d13071",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.sentence:\n",
    "    if(i== None):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eeff0b-b99d-4a14-a669-52cbf5ea3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'main_bot'\n",
    "os.makedirs(output_file, exist_ok=True)\n",
    "            \n",
    "#eda\n",
    "print('eda phase')\n",
    "\n",
    "file_name = 'eda.tsv'\n",
    "output_dir = os.path.join(output_file, file_name)\n",
    "gen_eda(df,output_dir , alpha=alpha, num_aug=num_aug , reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad1097-5f9f-4bd0-b7ad-f9243c5f00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import gen_eda\n",
    "import pymongo\n",
    "from cPosGpt2 import train_cposgpt2_and_augment\n",
    "\n",
    "MAX_LEN = 40\n",
    "epoch =40 # number of epochs\n",
    "\n",
    "scl = True  # if True -> scl + cross entropy loss. else just cross entropy loss\n",
    "temprature = 0.3  # temprature for contrastive loss\n",
    "lam = 0.9  # lambda for loss\n",
    "patience=12 # early stop\n",
    "hidden = 512\n",
    "seed = 49    # seed\n",
    "\n",
    "# for eda\n",
    "alpha = 0.1\n",
    "num_aug = 6\n",
    "# for pos_gpt2\n",
    "second_num_aug = 4\n",
    "\n",
    "\n",
    "# original datasets plus new_responses \n",
    "\n",
    "\n",
    "token2 = 'mongodb+srv://mongo:mongo@cluster0.gcj8po2.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n",
    "\n",
    "        \n",
    "myclient = pymongo.MongoClient(token2)\n",
    "\n",
    "\n",
    "mycol = mongodb_atlas('training_data')\n",
    "x = mycol.find()    \n",
    "df = pd.DataFrame(list(x))\n",
    "df = df.rename(columns={'tag':'label','patterns':'sentence'})\n",
    "df = df[['label','sentence']]\n",
    "\n",
    "\n",
    "    \n",
    "#main bot augmentation file\n",
    "output_file = 'main_bot'\n",
    "os.makedirs(output_file, exist_ok=True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "set_seed(seed)\n",
    "print('pre-process phase')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True) \n",
    "train_dataloader, val_dataloader = prepare_data(tokenizer,df,None,None, sample_num=10\\\n",
    "                                                                     , seed = seed , all=True,aug=False,aug_num = num_aug*second_num_aug)\n",
    "\n",
    "# without contrasive loss\n",
    "\n",
    "bert_classifier, optimizer = initialize_model(QModel_Classifier,hidden=hidden,num_labels = num_classes)\n",
    "scl = False\n",
    "\n",
    "print('training phase')\n",
    "val_loss, val_accuracy = train(bert_classifier,optimizer, train_dataloader, temprature, lam, scl, epoch ,val_dataloader, evaluation=True,patience=patience)\n",
    "\n",
    "#test \n",
    "#test_accuracy = test_evaluate(QModel_Classifier,cross_model_path, test_dataloader,hidden=hidden,num_labels=num_classes,feature_remove_max=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8265176-35e3-40be-9e54-7ae636835f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_path = f'main_bot/posgpt2_eda.tsv'\n",
    "\n",
    "df = pd.read_csv(aug_path,sep ='\\t')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4be5e-b7f2-4667-afb9-6cc61d789351",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_path = f'main_bot/eda.tsv'\n",
    "df = pd.read_csv(aug_path,sep ='\\t')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd556c0e-c8e9-4fb4-be22-14eea6927a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "class Args():\n",
    "    embedding_dim = 1024\n",
    "    hidden=512 \n",
    "    num_labels = 24\n",
    "    dropout=0.1\n",
    "    \n",
    "args = Args()\n",
    "PATH = r\"itmo_model.pt\"\n",
    "model = QModel_Classifier(args.embedding_dim,args.hidden,args.num_labels,args.dropout)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc145eb-179f-44a6-aa40-4f3c45ccbe64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6065b0-00f7-46d1-9ee2-77a1215d7ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69a228-6524-4be5-aaf1-e1547338b493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
